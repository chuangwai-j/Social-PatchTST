#!/usr/bin/env python3
"""
è®¡ç®—è®­ç»ƒé›†å…¨å±€ç»Ÿè®¡é‡ï¼ˆä¸»è§’+é…è§’ï¼‰
åªä½¿ç”¨è®­ç»ƒé›†æ•°æ®ï¼Œé¿å…æ•°æ®æ³„éœ²
"""

import pandas as pd
import numpy as np
import os
import json
from pathlib import Path
import time
from tqdm import tqdm

def process_features(df):
    """å¤„ç†ç‰¹å¾ï¼šæå–åŸºæœ¬ç‰¹å¾å¹¶è®¡ç®—é€Ÿåº¦å‘é‡"""
    lat = df['latitude'].values
    lon = df['longitude'].values
    flight_level = df['flight_level'].values
    ground_speed = df['ground_speed'].values
    track_angle = df['track_angle'].values
    vertical_rate = df['vertical_rate'].values
    selected_altitude = df['selected_altitude'].values

    # è½¬æ¢é€Ÿåº¦å‘é‡ (m/s)
    track_rad = np.deg2rad(track_angle)
    vx = ground_speed * np.sin(track_rad)
    vy = ground_speed * np.cos(track_rad)

    # ç»„åˆä¸»è¦ç‰¹å¾ [lat, lon, flight_level, vx, vy]
    processed_features = np.column_stack([
        lat, lon, flight_level, vx, vy
    ])

    # é¢å¤–ç‰¹å¾ [vertical_rate, selected_altitude]
    extra_features = np.column_stack([
        vertical_rate, selected_altitude
    ])

    return processed_features, extra_features

def calculate_training_statistics(train_paths_file, max_scenes=None, save_path="train_statistics.json"):
    """
    è®¡ç®—è®­ç»ƒé›†å…¨å±€ç»Ÿè®¡é‡ï¼ˆåªç»Ÿè®¡ä¸»è§’æ•°æ®ï¼‰

    Args:
        train_paths_file: è®­ç»ƒè·¯å¾„æ–‡ä»¶
        max_scenes: æœ€å¤§åœºæ™¯æ•°é™åˆ¶ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
        save_path: ç»Ÿè®¡é‡ä¿å­˜è·¯å¾„
    """
    print("ğŸš€ å¼€å§‹è®¡ç®—è®­ç»ƒé›†å…¨å±€ç»Ÿè®¡é‡...")
    print(f"   è·¯å¾„æ–‡ä»¶: {train_paths_file}")
    print(f"   ä¿å­˜è·¯å¾„: {save_path}")
    print("   æ³¨æ„ï¼šåªç»Ÿè®¡ä¸»è§’(ego)æ•°æ®ï¼Œä¸åŒ…æ‹¬é…è§’")
    print("   ç»Ÿè®¡ç‰¹å¾ï¼šä¸»è¦5ä¸ª + é¢å¤–2ä¸ª(vertical_rate, selected_altitude)")

    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡ - ä¸»è¦ç‰¹å¾ï¼ˆ5ä¸ªï¼‰
    feature_sum = np.zeros(5)  # 5ä¸ªç‰¹å¾çš„ç´¯è®¡å’Œ
    feature_sq_sum = np.zeros(5)  # å¹³æ–¹ç´¯è®¡å’Œ

    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡ - é¢å¤–ç‰¹å¾ï¼ˆ2ä¸ªï¼‰
    extra_sum = np.zeros(2)  # vertical_rate, selected_altitudeçš„ç´¯è®¡å’Œ
    extra_sq_sum = np.zeros(2)  # å¹³æ–¹ç´¯è®¡å’Œ

    total_count = 0
    valid_scenes = 0
    failed_scenes = 0

    # è¯»å–è®­ç»ƒåœºæ™¯è·¯å¾„
    scene_paths = []
    with open(train_paths_file, 'r') as f:
        for line in f:
            scene_path = line.strip()
            if scene_path:
                scene_paths.append(scene_path)

    if max_scenes:
        scene_paths = scene_paths[:max_scenes]
        print(f"   é™åˆ¶åœºæ™¯æ•°: {max_scenes}")

    print(f"   æ€»åœºæ™¯æ•°: {len(scene_paths)}")

    # é€ä¸ªåœºæ™¯å¤„ç†
    start_time = time.time()

    for i, scene_path in enumerate(tqdm(scene_paths, desc="å¤„ç†åœºæ™¯")):
        try:
            # åªå¤„ç†ä¸»è§’æ•°æ®
            ego_path = os.path.join(scene_path, "ego.csv")
            if not os.path.exists(ego_path):
                failed_scenes += 1
                continue

            ego_df = pd.read_csv(ego_path)
            main_features, extra_features = process_features(ego_df)  # [seq_len, 5] å’Œ [seq_len, 2]

            # ç´¯è®¡ä¸»è¦ç‰¹å¾ç»Ÿè®¡é‡ï¼ˆåªç»Ÿè®¡ä¸»è§’ï¼‰
            feature_sum += np.sum(main_features, axis=0)
            feature_sq_sum += np.sum(main_features ** 2, axis=0)

            # ç´¯è®¡é¢å¤–ç‰¹å¾ç»Ÿè®¡é‡
            extra_sum += np.sum(extra_features, axis=0)
            extra_sq_sum += np.sum(extra_features ** 2, axis=0)

            total_count += main_features.shape[0]
            valid_scenes += 1

        except Exception as e:
            failed_scenes += 1
            if failed_scenes <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
                print(f"   âš ï¸  åœºæ™¯ {i+1} å¤„ç†å¤±è´¥: {e}")
            continue

    end_time = time.time()

    # è®¡ç®—å…¨å±€å‡å€¼å’Œæ ‡å‡†å·®
    if total_count > 0:
        # ä¸»è¦ç‰¹å¾ç»Ÿè®¡é‡
        main_mean = feature_sum / total_count
        main_variance = (feature_sq_sum / total_count) - (main_mean ** 2)
        main_std = np.sqrt(np.maximum(main_variance, 1e-8))  # é¿å…è´Ÿæ•°æˆ–é›¶

        # é¢å¤–ç‰¹å¾ç»Ÿè®¡é‡
        extra_mean = extra_sum / total_count
        extra_variance = (extra_sq_sum / total_count) - (extra_mean ** 2)
        extra_std = np.sqrt(np.maximum(extra_variance, 1e-8))  # é¿å…è´Ÿæ•°æˆ–é›¶

        # ç»„ç»‡ç»Ÿè®¡é‡
        statistics = {
            "data_info": {
                "total_scenes": len(scene_paths),
                "valid_scenes": valid_scenes,
                "failed_scenes": failed_scenes,
                "total_data_points": int(total_count),
                "processing_time_seconds": end_time - start_time
            },
            "main_features": {
                "feature_names": ["latitude", "longitude", "flight_level", "vx", "vy"],
                "mean": main_mean.tolist(),
                "std": main_std.tolist()
            },
            "extra_features": {
                "feature_names": ["vertical_rate", "selected_altitude"],
                "mean": extra_mean.tolist(),
                "std": extra_std.tolist()
            },
            "all_features": {
                "feature_names": ["latitude", "longitude", "flight_level", "vx", "vy", "vertical_rate", "selected_altitude"],
                "mean": np.concatenate([main_mean, extra_mean]).tolist(),
                "std": np.concatenate([main_std, extra_std]).tolist()
            }
        }

        # ä¿å­˜ç»Ÿè®¡é‡
        if save_path:
            save_dir = os.path.dirname(save_path)
            if save_dir:  # åªæœ‰ç›®å½•è·¯å¾„ä¸ä¸ºç©ºæ—¶æ‰åˆ›å»º
                os.makedirs(save_dir, exist_ok=True)
            with open(save_path, 'w') as f:
                json.dump(statistics, f, indent=2)

            print(f"\nğŸ’¾ ç»Ÿè®¡é‡å·²ä¿å­˜åˆ°: {save_path}")

        print(f"\nâœ… è®¡ç®—å®Œæˆ!")
        print(f"   æœ‰æ•ˆåœºæ™¯: {valid_scenes:,} / {len(scene_paths):,}")
        print(f"   æ€»æ•°æ®ç‚¹: {total_count:,}")
        print(f"   å¤„ç†æ—¶é—´: {end_time - start_time:.1f} ç§’")
        print(f"   å¹³å‡æ¯åœºæ™¯: {(end_time - start_time) / valid_scenes:.3f} ç§’")

        print(f"\nğŸ“Š ä¸»è¦ç‰¹å¾ç»Ÿè®¡ç»“æœ:")
        main_feature_names = ["latitude(çº¬åº¦)", "longitude(ç»åº¦)", "flight_level(é«˜åº¦)", "vx(ä¸œå‘é€Ÿåº¦)", "vy(åŒ—å‘é€Ÿåº¦)"]
        for i, name in enumerate(main_feature_names):
            print(f"   {name:12}: å‡å€¼={main_mean[i]:8.3f}, æ ‡å‡†å·®={main_std[i]:8.3f}")

        print(f"\nğŸ“Š é¢å¤–ç‰¹å¾ç»Ÿè®¡ç»“æœ:")
        extra_feature_names = ["vertical_rate(å‚ç›´é€Ÿåº¦)", "selected_altitude(é€‰ä¸­é«˜åº¦)"]
        for i, name in enumerate(extra_feature_names):
            print(f"   {name:16}: å‡å€¼={extra_mean[i]:8.3f}, æ ‡å‡†å·®={extra_std[i]:8.3f}")

        return statistics
    else:
        print(f"âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼")
        return None

def test_statistics(saved_stats_path, test_sample_size=10):
    """æµ‹è¯•ç»Ÿè®¡é‡æ˜¯å¦æ­£ç¡®"""
    print(f"\nğŸ§ª æµ‹è¯•ç»Ÿè®¡é‡...")

    with open(saved_stats_path, 'r') as f:
        stats = json.load(f)

    # ä½¿ç”¨ä¸»è¦ç‰¹å¾è¿›è¡Œæµ‹è¯•
    main_mean = np.array(stats['main_features']['mean'])
    main_std = np.array(stats['main_features']['std'])
    extra_mean = np.array(stats['extra_features']['mean'])
    extra_std = np.array(stats['extra_features']['std'])

    # åŠ è½½ä¸€ä¸ªæµ‹è¯•åœºæ™¯
    train_paths_file = "/mnt/f/adsb/scenes_picked/train_paths.txt"
    with open(train_paths_file, 'r') as f:
        scene_paths = [line.strip() for line in f if line.strip()]

    # éšæœºé‡‡æ ·å‡ ä¸ªåœºæ™¯æµ‹è¯•
    import random
    test_paths = random.sample(scene_paths, min(test_sample_size, len(scene_paths)))

    for i, scene_path in enumerate(test_paths[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
        try:
            ego_path = os.path.join(scene_path, "ego.csv")
            ego_df = pd.read_csv(ego_path)
            main_features, extra_features = process_features(ego_df)

            # æ ‡å‡†åŒ–
            main_normalized = (main_features - main_mean) / main_std
            extra_normalized = (extra_features - extra_mean) / extra_std

            print(f"   åœºæ™¯ {i+1}:")
            print(f"     ä¸»è¦ç‰¹å¾èŒƒå›´: [{main_features.min():.2f}, {main_features.max():.2f}] -> "
                  f"æ ‡å‡†åŒ–èŒƒå›´: [{main_normalized.min():.2f}, {main_normalized.max():.2f}]")
            print(f"     é¢å¤–ç‰¹å¾èŒƒå›´: [{extra_features.min():.2f}, {extra_features.max():.2f}] -> "
                  f"æ ‡å‡†åŒ–èŒƒå›´: [{extra_normalized.min():.2f}, {extra_normalized.max():.2f}]")

        except Exception as e:
            print(f"   åœºæ™¯ {i+1}: æµ‹è¯•å¤±è´¥ - {e}")

def main():
    """ä¸»å‡½æ•°"""
    # è®­ç»ƒè·¯å¾„æ–‡ä»¶
    train_paths_file = "/mnt/f/adsb/scenes_picked/train_paths.txt"

    # å…ˆæµ‹è¯•10ä¸ªåœºæ™¯ï¼ŒéªŒè¯ä¿å­˜æ˜¯å¦æ­£å¸¸
    print("ğŸ§ª å…ˆæµ‹è¯•10ä¸ªåœºæ™¯...")
    stats = calculate_training_statistics(
        train_paths_file,
        max_scenes=10,  # å…ˆæµ‹è¯•10ä¸ªåœºæ™¯
        save_path="train_statistics_test_10.json"
    )

    if stats:
        print(f"\nâœ… æµ‹è¯•æˆåŠŸï¼")
        print(f"   10ä¸ªåœºæ™¯ç»Ÿè®¡é‡å·²ä¿å­˜åˆ°: train_statistics_test_10.json")
        print(f"   ç°åœ¨å¯ä»¥å¼€å§‹è®¡ç®—å…¨éƒ¨175,000ä¸ªåœºæ™¯äº†")

        # è¯¢é—®æ˜¯å¦ç»§ç»­è®¡ç®—å…¨éƒ¨
        user_input = input("\næ˜¯å¦ç»§ç»­è®¡ç®—å…¨éƒ¨175,000ä¸ªåœºæ™¯ï¼Ÿ(y/n): ")
        if user_input.lower() == 'y':
            print("\nğŸš€ å¼€å§‹è®¡ç®—å…¨éƒ¨è®­ç»ƒé›†ç»Ÿè®¡é‡...")
            print("   è¿™å°†ç»Ÿè®¡175,000ä¸ªåœºæ™¯çš„ä¸»è§’(ego)æ•°æ®")
            print("   é¢„è®¡éœ€è¦çº¦57åˆ†é’Ÿ")

            stats = calculate_training_statistics(
                train_paths_file,
                max_scenes=None,  # Noneè¡¨ç¤ºè®¡ç®—å…¨éƒ¨åœºæ™¯
                save_path="train_statistics_ego_only.json"
            )

            if stats:
                # æµ‹è¯•ç»Ÿè®¡é‡
                test_statistics("train_statistics_ego_only.json")
                print(f"\nğŸ’¾ å…¨é‡ç»Ÿè®¡é‡å·²ä¿å­˜åˆ°: train_statistics_ego_only.json")
                print(f"   å¯ä»¥åœ¨scene_dataset.pyä¸­åŠ è½½å¹¶ä½¿ç”¨è¿™äº›ç»Ÿè®¡é‡è¿›è¡Œæ ‡å‡†åŒ–")
        else:
            print("å·²å–æ¶ˆå…¨é‡è®¡ç®—")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯")

if __name__ == "__main__":
    main()