#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆåˆ†å±‚é‡‡æ ·å™¨ - è¯»å–é¢„ç”Ÿæˆç´¢å¼•
30 % Solo | 50 % Low-Risk | 20 % High-Risk
ä½¿ç”¨ç´¢å¼•æ–‡ä»¶ï¼Œé¿å…ç›®å½•æ‰«æï¼Œç§’çº§å®Œæˆ
"""
import time, pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split

SCENE_ROOT = Path("/mnt/f/adsb/scenes")
INDEX_FILE = Path("/mnt/f/adsb/scene_index.tsv")
OUTPUT_DIR = Path("/mnt/f/adsb/stratified_250k")

# åˆ†å±‚é˜ˆå€¼
SOLO_THR = 50.0
LOW_RISK_LO = 3.0
LOW_RISK_HI = 10.0

# å‚æ•°
TOTAL_TARGET = 250_000
TRAIN_RATIO = 0.70
VAL_RATIO = 0.15
TEST_RATIO = 0.15

def load_scenes_from_index():
    """ä»Žç´¢å¼•æ–‡ä»¶åŠ è½½åœºæ™¯æ•°æ®"""
    if not INDEX_FILE.exists():
        raise FileNotFoundError(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {INDEX_FILE}\nè¯·å…ˆè¿è¡Œ: bash data/scene_create/generate_index.sh")

    print("ðŸ“‚ è¯»å–åœºæ™¯ç´¢å¼•...")
    start_time = time.time()

    # è¯»å–ç´¢å¼•æ–‡ä»¶
    df_index = pd.read_csv(INDEX_FILE, sep='|', names=['scene_id', 'mindist_nm'])
    load_time = time.time() - start_time

    print(f"âœ… ç´¢å¼•è½½å…¥å®Œæˆï¼š{len(df_index):,} æ¡ | è€—æ—¶: {load_time:.2f}ç§’")

    # è½¬æ¢ä¸ºå®Œæ•´è®°å½•
    print("ðŸ”„ è½¬æ¢ä¸ºè®­ç»ƒæ•°æ®æ ¼å¼...")
    convert_start = time.time()

    records = []
    for _, row in df_index.iterrows():
        mindist = float(row.mindist_nm)
        scene_id = row.scene_id

        # åˆ†å±‚é€»è¾‘
        if mindist > SOLO_THR:
            layer = 'Solo'
        elif LOW_RISK_LO <= mindist <= LOW_RISK_HI:
            layer = 'Low-Risk'
        elif mindist < 3.0:
            layer = 'High-Risk'
        else:
            layer = 'Solo'  # å…œåº•

        records.append({
            'scene_id': scene_id,
            'layer': layer,
            'mindist_nm': mindist,
            'ego_path': str(SCENE_ROOT / scene_id / "ego.csv"),
            'neighbor_path': str(SCENE_ROOT / scene_id / "neighbors.csv"),
        })

    convert_time = time.time() - convert_start
    print(f"âœ… æ•°æ®è½¬æ¢å®Œæˆï¼š{len(records):,} æ¡ | è€—æ—¶: {convert_time:.2f}ç§’")

    return records, load_time, convert_time

def main():
    print("ðŸš€ ä¼˜åŒ–ç‰ˆåˆ†å±‚é‡‡æ ·å™¨å¯åŠ¨ (ä½¿ç”¨ç´¢å¼•æ–‡ä»¶)")
    print("="*60)

    # è®°å½•æ€»å¼€å§‹æ—¶é—´
    total_start_time = time.time()

    # 1. åŠ è½½æ•°æ®
    records, load_time, convert_time = load_scenes_from_index()
    df_all = pd.DataFrame(records)

    # 2. æ£€æŸ¥åˆ†å±‚åˆ†å¸ƒ
    print(f"\nðŸ“Š å„å±‚åˆ†å¸ƒ:")
    layer_distribution = df_all['layer'].value_counts()
    for layer, count in layer_distribution.items():
        percentage = count / len(df_all) * 100
        print(f"  {layer}: {count:,} æ¡ ({percentage:.1f}%)")

    # 3. åˆ†å±‚é‡‡æ ·
    print(f"\nðŸŽ¯ å¼€å§‹åˆ†å±‚é‡‡æ ·...")
    sample_start = time.time()

    def sample_layer(g, n):
        return g.sample(n=n, replace=len(g) < n, random_state=42)

    layer_targets = {
        'Solo': int(TOTAL_TARGET * 0.30),
        'Low-Risk': int(TOTAL_TARGET * 0.50),
        'High-Risk': int(TOTAL_TARGET * 0.20),
    }

    print(f"ç›®æ ‡é‡‡æ ·: Solo {layer_targets['Solo']:,} | Low-Risk {layer_targets['Low-Risk']:,} | High-Risk {layer_targets['High-Risk']:,}")

    sampled = (df_all.groupby('layer', group_keys=False)
                     .apply(lambda g: sample_layer(g, layer_targets[g.name])))

    sample_time = time.time() - sample_start
    print(f"âœ… é‡‡æ ·å®Œæˆï¼š{len(sampled):,} æ¡ | è€—æ—¶: {sample_time:.2f}ç§’")

    # 4. åˆ’åˆ†æ•°æ®é›†
    print(f"\nðŸŽ¯ åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†...")
    split_start = time.time()

    train, temp = train_test_split(sampled, stratify=sampled['layer'],
                                   train_size=TRAIN_RATIO, random_state=42)
    val, test = train_test_split(temp, stratify=temp['layer'],
                                 train_size=VAL_RATIO/(VAL_RATIO+TEST_RATIO), random_state=42)

    split_time = time.time() - split_start
    print(f"âœ… æ•°æ®åˆ’åˆ†å®Œæˆ | è€—æ—¶: {split_time:.2f}ç§’")

    # 5. è¾“å‡ºCSV
    print(f"\nðŸ’¾ ä¿å­˜CSVæ–‡ä»¶...")
    output_start = time.time()

    OUTPUT_DIR.mkdir(exist_ok=True)
    train.to_csv(OUTPUT_DIR / "train.csv", index=False)
    val.to_csv(OUTPUT_DIR / "val.csv", index=False)
    test.to_csv(OUTPUT_DIR / "test.csv", index=False)

    output_time = time.time() - output_start
    total_time = time.time() - total_start_time

    # 6. ç»Ÿè®¡æŠ¥å‘Š
    print(f"\n" + "="*60)
    print(f"ðŸŽ‰ ä¼˜åŒ–ç‰ˆ 25 ä¸‡æ¡åˆ†å±‚é‡‡æ ·å®Œæˆ")
    print(f"="*60)

    for name, df in zip(('Train', 'Val', 'Test'), (train, val, test)):
        layer_counts = df['layer'].value_counts()
        print(f"{name:6s}: {len(df):,} æ¡ | åˆ†å±‚æ¯”ä¾‹: ", end="")
        for layer in ['Solo', 'Low-Risk', 'High-Risk']:
            count = layer_counts.get(layer, 0)
            pct = count / len(df) * 100
            print(f"{layer} {pct:.0f}% ", end="")
        print()

    print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
    print(f"   ç´¢å¼•åŠ è½½: {load_time:.2f}ç§’")
    print(f"   æ•°æ®è½¬æ¢: {convert_time:.2f}ç§’")
    print(f"   åˆ†å±‚é‡‡æ ·: {sample_time:.2f}ç§’")
    print(f"   æ•°æ®åˆ’åˆ†: {split_time:.2f}ç§’")
    print(f"   CSVè¾“å‡º: {output_time:.2f}ç§’")
    print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")

    print(f"\nðŸ“‚ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"âœ… æ•°æ®å·²å°±ç»ªï¼Œå¯ç›´æŽ¥å¼€å§‹è®­ç»ƒï¼")

if __name__ == "__main__":
    try:
        main()
    except FileNotFoundError as e:
        print(e)
        print(f"\nðŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print(f"   1. å…ˆè¿è¡Œ: bash data/scene_create/generate_index.sh")
        print(f"   2. ç„¶åŽè¿è¡Œ: python data/scene_create/stratified_sampler.py")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")