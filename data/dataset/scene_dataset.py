"""
Social-PatchTST åœºæ™¯æ•°æ®é›†åŠ è½½å™¨
æ”¯æŒä»CSVæ–‡ä»¶åŠ è½½åˆ†å±‚é‡‡æ ·çš„åœºæ™¯æ•°æ®ï¼Œå¯ç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒ
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from typing import Tuple
import os
from pathlib import Path
import warnings
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config.config_manager import load_config

warnings.filterwarnings('ignore')


class PrecomputedScaler:
    """ä½¿ç”¨é¢„è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®çš„æ ‡å‡†åŒ–å™¨"""

    def __init__(self, mean: np.ndarray, std: np.ndarray):
        """
        åˆå§‹åŒ–é¢„è®¡ç®—æ ‡å‡†åŒ–å™¨

        Args:
            mean: ç‰¹å¾å‡å€¼æ•°ç»„
            std: ç‰¹å¾æ ‡å‡†å·®æ•°ç»„
        """
        self.mean_ = mean.astype(np.float64)
        self.scale_ = std.astype(np.float64)

        # é¿å…é™¤é›¶é”™è¯¯
        self.scale_[self.scale_ == 0] = 1.0

    def transform(self, X: np.ndarray) -> np.ndarray:
        """
        åº”ç”¨æ ‡å‡†åŒ–å˜æ¢

        Args:
            X: è¾“å…¥æ•°æ® [n_samples, n_features]

        Returns:
            æ ‡å‡†åŒ–åçš„æ•°æ®
        """
        return (X.astype(np.float64) - self.mean_) / self.scale_

    def fit_transform(self, X: np.ndarray) -> np.ndarray:
        """å…¼å®¹æ–¹æ³•ï¼Œç›´æ¥è¿”å›transformç»“æœ"""
        return self.transform(X)

    def inverse_transform(self, X: np.ndarray) -> np.ndarray:
        """
        åå‘æ ‡å‡†åŒ–å˜æ¢

        Args:
            X: æ ‡å‡†åŒ–åçš„æ•°æ®

        Returns:
            åæ ‡å‡†åŒ–åçš„æ•°æ®
        """
        return (X.astype(np.float64) * self.scale_) + self.mean_

# å®é™…CSVä¸­çš„åˆ—åå®šä¹‰
CSV_FEATURE_COLUMNS = {
    'temporal_features': [
        'latitude', 'longitude',      # å±€éƒ¨ ENU (m)
        'flight_level',              # æ°”å‹é«˜åº¦(ç±³)
        'ground_speed', 'track_angle'  # å°†ç”¨äºè®¡ç®— vx, vy (m/s)
    ],
    'spatial_features': ['latitude', 'longitude'],
    'target_features': ['latitude', 'longitude'],
    'static_features': ['aircraft_type', 'callsign', 'target_address']
}


class SocialPatchTSTDataset(Dataset):
    """
    Social-PatchTST åœºæ™¯æ•°æ®é›†
    ç›´æ¥ä»train/val/testæ–‡ä»¶å¤¹ä¸­æŒ‰é¡ºåºè¯»å–åœºæ™¯æ•°æ®
    """

    def __init__(self, data_dir: str, max_neighbors: int = 20, sequence_length: int = 600, paths_file: str = None):
        """
        ä»æ•°æ®ç›®å½•åˆå§‹åŒ–åœºæ™¯æ•°æ®é›†

        Args:
            data_dir: æ•°æ®æ ¹ç›®å½•è·¯å¾„
            max_neighbors: æœ€å¤§é‚»å±…æ•°é‡
            sequence_length: åºåˆ—é•¿åº¦
            paths_file: è·¯å¾„æ–‡ä»¶txt (train_paths.txt, val_paths.txt, test_paths.txt)
        """
        self.data_dir = Path(data_dir)
        self.max_neighbors = max_neighbors
        self.sequence_length = sequence_length

        # è·å–ç‰¹å¾åˆ—å®šä¹‰
        self.temporal_features = CSV_FEATURE_COLUMNS['temporal_features']
        self.spatial_features = CSV_FEATURE_COLUMNS['spatial_features']
        self.target_features = CSV_FEATURE_COLUMNS['target_features']

        print(f"ğŸ“‚ ä»è·¯å¾„æ–‡ä»¶åŠ è½½åœºæ™¯: {paths_file}")
        # é«˜æ•ˆè¯»å–è·¯å¾„æ–‡ä»¶ï¼Œå»é‡å¤„ç†
        self.scenes = []
        seen_scenes = set()  # é˜²é‡å¤

        if paths_file and os.path.exists(paths_file):
            with open(paths_file, 'r') as f:
                for line_num, line in enumerate(f):
                    scene_path = line.strip()
                    if not scene_path:
                        continue

                    scene_name = os.path.basename(scene_path)

                    # é˜²é‡å¤æ£€æŸ¥
                    if scene_name in seen_scenes:
                        continue
                    seen_scenes.add(scene_name)

                    ego_path = os.path.join(scene_path, "ego.csv")
                    neighbor_path = os.path.join(scene_path, "neighbors.csv")

                    self.scenes.append({
                        'scene_id': scene_name,
                        'ego_path': ego_path,
                        'neighbor_path': neighbor_path,
                        'layer': self._extract_layer_from_name(scene_name)
                    })

                    # å‡å°‘æ‰“å°é¢‘ç‡ - æ¯50kä¸ªåœºæ™¯æ‰“å°ä¸€æ¬¡
                    if len(self.scenes) % 50000 == 0:
                        print(f"   å·²åŠ è½½ {len(self.scenes)} ä¸ªå”¯ä¸€åœºæ™¯...")

        print(f"âœ… å‘ç° {len(self.scenes)} ä¸ªå”¯ä¸€åœºæ™¯")

        # å¿«é€ŸéªŒè¯æ•°æ®å®Œæ•´æ€§
        print("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        self._verify_data_integrity()

        # åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
        self._initialize_scalers()

    def _extract_layer_from_name(self, scene_name: str) -> str:
        """ä»åœºæ™¯åç§°ä¸­æå–å±‚çº§ä¿¡æ¯"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®ä½ çš„å‘½åè§„åˆ™æ¥æå–å±‚çº§
        # æš‚æ—¶è¿”å›é»˜è®¤å€¼
        return "default"

    def _verify_data_integrity(self):
        """è·³è¿‡æ•°æ®å®Œæ•´æ€§éªŒè¯ï¼Œé¿å…æ‰«ææ–‡ä»¶å¤¹"""
        print("âš¡ è·³è¿‡å®Œæ•´æ€§éªŒè¯ï¼Œç›´æ¥ä½¿ç”¨è·¯å¾„æ–‡ä»¶ä¸­çš„åœºæ™¯")
        self.valid_scenes = self.scenes
        print(f"âœ… ç›´æ¥ä½¿ç”¨å…¨éƒ¨åœºæ™¯æ•°é‡: {len(self.valid_scenes)}")

    def _initialize_scalers(self):
        """ä»é…ç½®æ–‡ä»¶åˆå§‹åŒ–æ•°æ®æ ‡å‡†åŒ–å™¨ï¼Œä½¿ç”¨é¢„è®¡ç®—çš„ç»Ÿè®¡ä¿¡æ¯"""
        print("ğŸ”§ ä»é…ç½®æ–‡ä»¶åˆå§‹åŒ–æ•°æ®æ ‡å‡†åŒ–å™¨...")

        try:
            # åŠ è½½é…ç½®æ–‡ä»¶
            config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
                                     "config", "social_patchtst_config.yaml")
            config = load_config(config_path)

            # è·å–ç»Ÿè®¡æ•°æ®
            statistics = config.get('data.statistics', {})

            if not statistics:
                print("âš ï¸  é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç»Ÿè®¡ä¿¡æ¯ï¼Œå°†ä½¿ç”¨åŸå§‹æ•°æ®")
                self.feature_scaler = None
                return

            # ä½¿ç”¨ä¸»è¦ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯ (latitude, longitude, flight_level, vx, vy)
            main_stats = statistics.get('main_features', {})

            if not main_stats.get('mean') or not main_stats.get('std'):
                print("âš ï¸  é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ä¸»è¦ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œå°†ä½¿ç”¨åŸå§‹æ•°æ®")
                self.feature_scaler = None
                return

            # åˆ›å»ºè‡ªå®šä¹‰çš„æ ‡å‡†åŒ–å™¨ï¼Œä½¿ç”¨é¢„è®¡ç®—çš„å‡å€¼å’Œæ ‡å‡†å·®
            self.feature_scaler = PrecomputedScaler(
                mean=np.array(main_stats['mean']),
                std=np.array(main_stats['std'])
            )

            print(f"âœ… æ ‡å‡†åŒ–å™¨å·²ä»é…ç½®æ–‡ä»¶åŠ è½½")
            print(f"   ç‰¹å¾é¡ºåº: {main_stats['feature_names']}")
            print(f"   å‡å€¼: {main_stats['mean']}")
            print(f"   æ ‡å‡†å·®: {main_stats['std']}")

        except Exception as e:
            print(f"âš ï¸  ä»é…ç½®æ–‡ä»¶åŠ è½½ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            print("   å°†ä½¿ç”¨åŸå§‹æ•°æ®")
            self.feature_scaler = None

    def _process_features(self, df):
        """
        å¤„ç†ç‰¹å¾ï¼šæå–åŸºæœ¬ç‰¹å¾å¹¶è®¡ç®—é€Ÿåº¦å‘é‡
        Args:
            df: åŸå§‹DataFrame
        Returns:
            processed_features: å¤„ç†åçš„ç‰¹å¾æ•°ç»„ [seq_len, 5]
        """
        # æå–åŸºæœ¬ç‰¹å¾
        lat = df['latitude'].values
        lon = df['longitude'].values
        flight_level = df['flight_level'].values
        ground_speed = df['ground_speed'].values
        track_angle = df['track_angle'].values

        # è½¬æ¢é€Ÿåº¦å‘é‡ (m/s)
        # æ³¨æ„ï¼štrack_angle å•ä½æ˜¯åº¦ï¼Œéœ€è¦è½¬æ¢ä¸ºå¼§åº¦
        track_rad = np.deg2rad(track_angle)
        vx = ground_speed * np.sin(track_rad)  # ä¸œå‘é€Ÿåº¦
        vy = ground_speed * np.cos(track_rad)  # åŒ—å‘é€Ÿåº¦

        # ç»„åˆç‰¹å¾ [lat, lon, flight_level, vx, vy]
        processed_features = np.column_stack([
            lat, lon, flight_level, vx, vy
        ])

        return processed_features

    def _load_single_scene(self, idx):
        """åŠ è½½å•ä¸ªåœºæ™¯çš„æ•°æ®"""
        scene = self.valid_scenes[idx]
        scene_id = scene['scene_id']

        try:
            # åŠ è½½egoæ•°æ®
            ego_df = pd.read_csv(scene['ego_path'])
            ego_features = self._process_features(ego_df)  # [seq_len, 5]

            # åŠ è½½neighboræ•°æ®
            neighbors_df = pd.read_csv(scene['neighbor_path'])

            # é€‰æ‹©æœ€å¤šmax_neighborsä¸ªé‚»å±…ï¼ˆä¿æŒé¡ºåºï¼‰
            if len(neighbors_df) > self.max_neighbors:
                neighbors_df = neighbors_df.head(self.max_neighbors)

            neighbor_features_list = []
            # æŒ‰é£æœºIDåˆ†ç»„å¤„ç†é‚»å±…æ•°æ®
            for aircraft_id, neighbor_group in neighbors_df.groupby('target_address'):
                neighbor_features = self._process_features(neighbor_group)
                neighbor_features_list.append(neighbor_features)

            return {
                'scene_id': scene_id,
                'ego_features': ego_features,
                'neighbor_features': neighbor_features_list,
                'layer': scene['layer']
            }

        except Exception as e:
            print(f"âš ï¸  åŠ è½½åœºæ™¯ {scene_id} å¤±è´¥: {e}")
            return None

    def __len__(self):
        return len(self.valid_scenes)

    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ•°æ®æ ·æœ¬ï¼Œè½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›çš„æ ¼å¼"""
        scene_data = self._load_single_scene(idx)

        if scene_data is None:
            # è¿”å›ç©ºæ ·æœ¬
            n_aircrafts = self.max_neighbors + 1  # ego + neighbors
            seq_len = self.sequence_length
            n_temporal_features = len(self.temporal_features)

            return {
                'scene_id': f"empty_{idx}",
                'temporal': torch.zeros(n_aircrafts, seq_len, n_temporal_features),
                'spatial': torch.zeros(n_aircrafts, 2),  # lat, lon
                'targets': torch.zeros(n_aircrafts, 120, 4),  # pred_len, targets
                'distance_matrix': torch.eye(n_aircrafts),  # å•ä½çŸ©é˜µ
                'aircraft_ids': [f"empty_{i}" for i in range(n_aircrafts)],
                'layer': 'Unknown'
            }

        # å¤„ç†egoç‰¹å¾
        ego_features = scene_data['ego_features']  # [seq_len, 5]
        if self.feature_scaler is not None:
            ego_features = self.feature_scaler.transform(ego_features)

        # ç¡®ä¿åºåˆ—é•¿åº¦ä¸€è‡´
        if len(ego_features) > self.sequence_length:
            ego_features = ego_features[:self.sequence_length]
        elif len(ego_features) < self.sequence_length:
            padding = np.zeros((self.sequence_length - len(ego_features), ego_features.shape[1]))
            ego_features = np.vstack([ego_features, padding])

        # å¤„ç†é‚»å±…ç‰¹å¾
        neighbor_features_list = scene_data['neighbor_features']
        n_aircrafts = min(len(neighbor_features_list) + 1, self.max_neighbors + 1)  # +1 for ego

        # åˆå§‹åŒ–å¼ é‡
        temporal_data = torch.zeros(n_aircrafts, self.sequence_length, len(self.temporal_features))
        spatial_data = torch.zeros(n_aircrafts, 2)  # lat, lon
        aircraft_ids = ['ego']

        # ç¬¬0æ¶é£æœºæ˜¯ego
        temporal_data[0] = torch.from_numpy(ego_features).float()
        spatial_data[0] = torch.from_numpy(ego_features[-1, :2]).float()  # æœ€åä½ç½®çš„lat, lon

        # å¡«å……é‚»å±…æ•°æ®
        for i, neigh_feat in enumerate(neighbor_features_list[:self.max_neighbors]):
            if i + 1 >= n_aircrafts:
                break

            if neigh_feat.ndim == 1:
                neigh_feat = neigh_feat.reshape(1, -1)

            if self.feature_scaler is not None:
                neigh_feat = self.feature_scaler.transform(neigh_feat)

            if len(neigh_feat) > self.sequence_length:
                neigh_feat = neigh_feat[:self.sequence_length]
            elif len(neigh_feat) < self.sequence_length:
                padding = np.zeros((self.sequence_length - len(neigh_feat), neigh_feat.shape[1]))
                neigh_feat = np.vstack([neigh_feat, padding])

            temporal_data[i + 1] = torch.from_numpy(neigh_feat).float()
            spatial_data[i + 1] = torch.from_numpy(neigh_feat[-1, :2]).float()
            aircraft_ids.append(f"neighbor_{i}")

        # åˆ›å»ºè·ç¦»çŸ©é˜µ (åŸºäºå½“å‰ä½ç½®)
        distance_matrix = torch.zeros(n_aircrafts, n_aircrafts)
        for i in range(n_aircrafts):
            for j in range(n_aircrafts):
                if i != j:
                    # è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»
                    dist = torch.norm(spatial_data[i] - spatial_data[j])
                    distance_matrix[i, j] = dist

        # åˆ›å»ºç›®æ ‡æ•°æ® (åŸºäºæœ€åçš„ä½ç½®)
        # ç®€åŒ–ï¼šç›®æ ‡æ˜¯é¢„æµ‹æœªæ¥ä½ç½®ï¼Œè¿™é‡Œä½¿ç”¨æœ€åä½ç½®ä½œä¸ºç›®æ ‡åŸºç¡€
        last_position = temporal_data[:, -1, :4]  # [n_aircrafts, 4] - lat,lon,flight_level,vx
        targets = last_position.unsqueeze(1).repeat(1, 120, 1)  # [n_aircrafts, 120, 4]

        # è¿”å›æ•°æ®ï¼Œä¸è¦æ·»åŠ batchç»´åº¦ï¼ˆDataLoaderä¼šå¤„ç†ï¼‰
        return {
            'scene_id': scene_data['scene_id'],
            'temporal': temporal_data,  # [n_aircrafts, seq_len, features]
            'spatial': spatial_data,    # [n_aircrafts, 2]
            'targets': targets,         # [n_aircrafts, 120, 4]
            'distance_matrix': distance_matrix,  # [n_aircrafts, n_aircrafts]
            'aircraft_ids': aircraft_ids,  # List of IDs
            'layer': scene_data['layer']
        }


def create_social_patchtst_loaders(config_path: str = None, batch_size: int = 32,
                                  max_neighbors: int = 20, sequence_length: int = 600,
                                  num_workers: int = 4) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """
    åˆ›å»ºSocial-PatchTSTæ•°æ®åŠ è½½å™¨

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        batch_size: æ‰¹å¤§å°
        max_neighbors: æ¯ä¸ªåœºæ™¯æœ€å¤§é‚»å±…æ•°é‡
        sequence_length: åºåˆ—é•¿åº¦
        num_workers: æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°

    Returns:
        train_loader, val_loader, test_loader
    """
    # åŠ è½½é…ç½®æ–‡ä»¶
    if config_path is None:
        config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
                                 "config", "social_patchtst_config.yaml")

    config = load_config(config_path)

    # ä»é…ç½®æ–‡ä»¶è·å–æ•°æ®ç›®å½•
    scenes_dir = config.get('data.scenes_dir') or config.get('data.data_dir')
    if not scenes_dir:
        raise ValueError("é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ° data.scenes_dir æˆ– data.data_dir")

    scenes_path = Path(scenes_dir)

    # è·¯å¾„æ–‡ä»¶è·¯å¾„
    train_paths_file = scenes_path / "train_paths.txt"
    val_paths_file = scenes_path / "val_paths.txt"
    test_paths_file = scenes_path / "test_paths.txt"

    print("ğŸš€ åˆ›å»ºSocial-PatchTSTæ•°æ®åŠ è½½å™¨")
    print(f"   é…ç½®æ–‡ä»¶: {config_path}")
    print(f"   æ•°æ®ç›®å½•: {scenes_path}")
    print(f"   è®­ç»ƒè·¯å¾„: {train_paths_file}")
    print(f"   éªŒè¯è·¯å¾„: {val_paths_file}")
    print(f"   æµ‹è¯•è·¯å¾„: {test_paths_file}")

    # æ£€æŸ¥è·¯å¾„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not train_paths_file.exists():
        raise FileNotFoundError(f"è®­ç»ƒè·¯å¾„æ–‡ä»¶ä¸å­˜åœ¨: {train_paths_file}")
    if not val_paths_file.exists():
        raise FileNotFoundError(f"éªŒè¯è·¯å¾„æ–‡ä»¶ä¸å­˜åœ¨: {val_paths_file}")
    if not test_paths_file.exists():
        raise FileNotFoundError(f"æµ‹è¯•è·¯å¾„æ–‡ä»¶ä¸å­˜åœ¨: {test_paths_file}")

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = SocialPatchTSTDataset(str(scenes_path), max_neighbors, sequence_length, str(train_paths_file))
    val_dataset = SocialPatchTSTDataset(str(scenes_path), max_neighbors, sequence_length, str(val_paths_file))
    test_dataset = SocialPatchTSTDataset(str(scenes_path), max_neighbors, sequence_length, str(test_paths_file))

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        drop_last=True,
        persistent_workers=num_workers > 0,  # å¦‚æœæœ‰workerå°±ä¿æŒå­˜æ´»
        pin_memory=True  # åŠ é€ŸCPUåˆ°GPUä¼ è¾“
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        persistent_workers=num_workers > 0,
        pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        persistent_workers=num_workers > 0,
        pin_memory=True
    )

    print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ!")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬ ({len(train_loader)} batches)")
    print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬ ({len(val_loader)} batches)")
    print(f"   æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬ ({len(test_loader)} batches)")
    print(f"   ç‰¹å¾ç»´åº¦: {len(CSV_FEATURE_COLUMNS['temporal_features'])}")

    return train_loader, val_loader, test_loader


def get_feature_info():
    """è·å–ç‰¹å¾ä¿¡æ¯"""
    return {
        'temporal_features': CSV_FEATURE_COLUMNS['temporal_features'],
        'spatial_features': CSV_FEATURE_COLUMNS['spatial_features'],
        'target_features': CSV_FEATURE_COLUMNS['target_features'],
        'n_temporal_features': len(CSV_FEATURE_COLUMNS['temporal_features']),
        'n_target_features': len(CSV_FEATURE_COLUMNS['target_features'])
    }