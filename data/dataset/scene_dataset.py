"""
Social-PatchTST åœºæ™¯æ•°æ®é›†åŠ è½½å™¨
æ”¯æŒä»CSVæ–‡ä»¶åŠ è½½åˆ†å±‚é‡‡æ ·çš„åœºæ™¯æ•°æ®ï¼Œå¯ç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒ
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from typing import Tuple
import os
from pathlib import Path
import warnings
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config.config_manager import load_config

warnings.filterwarnings('ignore')

# å®é™…CSVä¸­çš„åˆ—åå®šä¹‰
CSV_FEATURE_COLUMNS = {
    'temporal_features': [
        'latitude', 'longitude',      # å±€éƒ¨ ENU (m)
        'flight_level',              # æ°”å‹é«˜åº¦(ç±³)
        'ground_speed', 'track_angle'  # å°†ç”¨äºè®¡ç®— vx, vy (m/s)
    ],
    'spatial_features': ['latitude', 'longitude'],
    'target_features': ['latitude', 'longitude'],
    'static_features': ['aircraft_type', 'callsign', 'target_address']
}


class SocialPatchTSTDataset(Dataset):
    """
    Social-PatchTST åœºæ™¯æ•°æ®é›†
    ç›´æ¥ä»train/val/testæ–‡ä»¶å¤¹ä¸­æŒ‰é¡ºåºè¯»å–åœºæ™¯æ•°æ®
    """

    def __init__(self, data_dir: str, max_neighbors: int = 20, sequence_length: int = 600, paths_file: str = None):
        """
        ä»æ•°æ®ç›®å½•åˆå§‹åŒ–åœºæ™¯æ•°æ®é›†

        Args:
            data_dir: æ•°æ®æ ¹ç›®å½•è·¯å¾„
            max_neighbors: æœ€å¤§é‚»å±…æ•°é‡
            sequence_length: åºåˆ—é•¿åº¦
            paths_file: è·¯å¾„æ–‡ä»¶txt (train_paths.txt, val_paths.txt, test_paths.txt)
        """
        self.data_dir = Path(data_dir)
        self.max_neighbors = max_neighbors
        self.sequence_length = sequence_length

        # è·å–ç‰¹å¾åˆ—å®šä¹‰
        self.temporal_features = CSV_FEATURE_COLUMNS['temporal_features']
        self.spatial_features = CSV_FEATURE_COLUMNS['spatial_features']
        self.target_features = CSV_FEATURE_COLUMNS['target_features']

        print(f"ğŸ“‚ ä»è·¯å¾„æ–‡ä»¶åŠ è½½åœºæ™¯: {paths_file}")
        # ä»txtæ–‡ä»¶è¯»å–åœºæ™¯è·¯å¾„
        self.scenes = []
        if paths_file and os.path.exists(paths_file):
            with open(paths_file, 'r') as f:
                for line in f:
                    scene_path = line.strip()
                    if scene_path:
                        scene_name = os.path.basename(scene_path)
                        ego_path = os.path.join(scene_path, "ego.csv")
                        neighbor_path = os.path.join(scene_path, "neighbors.csv")

                        if os.path.exists(ego_path) and os.path.exists(neighbor_path):
                            self.scenes.append({
                                'scene_id': scene_name,
                                'ego_path': ego_path,
                                'neighbor_path': neighbor_path,
                                'layer': self._extract_layer_from_name(scene_name)
                            })
        print(f"âœ… å‘ç° {len(self.scenes)} ä¸ªæœ‰æ•ˆåœºæ™¯")

        # å¿«é€ŸéªŒè¯æ•°æ®å®Œæ•´æ€§
        print("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        self._verify_data_integrity()

        # åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
        self._initialize_scalers()

    def _extract_layer_from_name(self, scene_name: str) -> str:
        """ä»åœºæ™¯åç§°ä¸­æå–å±‚çº§ä¿¡æ¯"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®ä½ çš„å‘½åè§„åˆ™æ¥æå–å±‚çº§
        # æš‚æ—¶è¿”å›é»˜è®¤å€¼
        return "default"

    def _verify_data_integrity(self):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        # æŠ½æ ·éªŒè¯å‰100ä¸ªåœºæ™¯
        sample_size = min(100, len(self.scenes))
        valid_count = 0

        for idx in range(sample_size):
            scene = self.scenes[idx]
            ego_path = scene['ego_path']
            neighbor_path = scene['neighbor_path']

            if os.path.exists(ego_path) and os.path.exists(neighbor_path):
                valid_count += 1

        validity_rate = valid_count / sample_size
        if validity_rate >= 0.9:
            print(f"âœ… æ•°æ®å®Œæ•´æ€§è‰¯å¥½ ({validity_rate:.1%})ï¼Œä½¿ç”¨å…¨éƒ¨åœºæ™¯")
            self.valid_scenes = self.scenes
        else:
            print(f"âš ï¸  æ•°æ®å®Œæ•´æ€§è¾ƒä½ ({validity_rate:.1%})ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®")
            self.valid_scenes = self.scenes  # ä»ä½¿ç”¨å…¨éƒ¨æ•°æ®

        print(f"æœ€ç»ˆä½¿ç”¨åœºæ™¯æ•°é‡: {len(self.valid_scenes)}")

    def _initialize_scalers(self):
        """åˆå§‹åŒ–æ•°æ®æ ‡å‡†åŒ–å™¨"""
        print("ğŸ”§ åˆå§‹åŒ–æ•°æ®æ ‡å‡†åŒ–å™¨...")

        sample_size = min(50, len(self.valid_scenes))
        all_features = []

        for i in range(sample_size):
            try:
                scene = self.valid_scenes[i]

                # åŠ è½½egoæ•°æ®å¹¶å¤„ç†ç‰¹å¾
                ego_df = pd.read_csv(scene['ego_path'])
                ego_features = self._process_features(ego_df)
                all_features.append(ego_features)

                # åŠ è½½é‚»å±…æ•°æ®æ ·æœ¬å¹¶å¤„ç†ç‰¹å¾
                neighbors_df = pd.read_csv(scene['neighbor_path'])
                # å¤„ç†å‰å‡ ä¸ªé‚»å±…æ¥æ”¶é›†ç‰¹å¾
                neighbor_groups = neighbors_df.groupby('target_address')
                for aircraft_id, neighbor_group in list(neighbor_groups)[:3]:  # é™åˆ¶ä¸ºå‰3ä¸ªé‚»å±…
                    neighbor_features = self._process_features(neighbor_group)
                    all_features.append(neighbor_features)

            except Exception as e:
                continue

        if all_features:
            all_features = np.vstack(all_features)
            self.feature_scaler = StandardScaler()
            self.feature_scaler.fit(all_features)
            print(f"âœ… æ ‡å‡†åŒ–å™¨å·²æ‹Ÿåˆï¼Œç‰¹å¾ç»´åº¦: {all_features.shape}")
        else:
            self.feature_scaler = None
            print("âš ï¸  æ— æ³•æ‹Ÿåˆæ ‡å‡†åŒ–å™¨ï¼Œå°†ä½¿ç”¨åŸå§‹æ•°æ®")

    def _process_features(self, df):
        """
        å¤„ç†ç‰¹å¾ï¼šæå–åŸºæœ¬ç‰¹å¾å¹¶è®¡ç®—é€Ÿåº¦å‘é‡
        Args:
            df: åŸå§‹DataFrame
        Returns:
            processed_features: å¤„ç†åçš„ç‰¹å¾æ•°ç»„ [seq_len, 5]
        """
        # æå–åŸºæœ¬ç‰¹å¾
        lat = df['latitude'].values
        lon = df['longitude'].values
        flight_level = df['flight_level'].values
        ground_speed = df['ground_speed'].values
        track_angle = df['track_angle'].values

        # è½¬æ¢é€Ÿåº¦å‘é‡ (m/s)
        # æ³¨æ„ï¼štrack_angle å•ä½æ˜¯åº¦ï¼Œéœ€è¦è½¬æ¢ä¸ºå¼§åº¦
        track_rad = np.deg2rad(track_angle)
        vx = ground_speed * np.sin(track_rad)  # ä¸œå‘é€Ÿåº¦
        vy = ground_speed * np.cos(track_rad)  # åŒ—å‘é€Ÿåº¦

        # ç»„åˆç‰¹å¾ [lat, lon, flight_level, vx, vy]
        processed_features = np.column_stack([
            lat, lon, flight_level, vx, vy
        ])

        return processed_features

    def _load_single_scene(self, idx):
        """åŠ è½½å•ä¸ªåœºæ™¯çš„æ•°æ®"""
        scene = self.valid_scenes[idx]
        scene_id = scene['scene_id']

        try:
            # åŠ è½½egoæ•°æ®
            ego_df = pd.read_csv(scene['ego_path'])
            ego_features = self._process_features(ego_df)  # [seq_len, 5]

            # åŠ è½½neighboræ•°æ®
            neighbors_df = pd.read_csv(scene['neighbor_path'])

            # é€‰æ‹©æœ€å¤šmax_neighborsä¸ªé‚»å±…ï¼ˆä¿æŒé¡ºåºï¼‰
            if len(neighbors_df) > self.max_neighbors:
                neighbors_df = neighbors_df.head(self.max_neighbors)

            neighbor_features_list = []
            # æŒ‰é£æœºIDåˆ†ç»„å¤„ç†é‚»å±…æ•°æ®
            for aircraft_id, neighbor_group in neighbors_df.groupby('target_address'):
                neighbor_features = self._process_features(neighbor_group)
                neighbor_features_list.append(neighbor_features)

            return {
                'scene_id': scene_id,
                'ego_features': ego_features,
                'neighbor_features': neighbor_features_list,
                'layer': scene['layer']
            }

        except Exception as e:
            print(f"âš ï¸  åŠ è½½åœºæ™¯ {scene_id} å¤±è´¥: {e}")
            return None

    def __len__(self):
        return len(self.valid_scenes)

    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ•°æ®æ ·æœ¬"""
        scene_data = self._load_single_scene(idx)

        if scene_data is None:
            # è¿”å›ç©ºæ ·æœ¬
            return {
                'scene_id': f"empty_{idx}",
                'ego_features': torch.zeros(self.sequence_length, 5),  # 5ç»´ç‰¹å¾
                'neighbor_features': torch.zeros(self.max_neighbors, self.sequence_length, 5),  # 5ç»´ç‰¹å¾
                'target': torch.zeros(2),  # [lat, lon]
                'layer': 'Unknown'
            }

        # å¤„ç†egoç‰¹å¾
        ego_features = scene_data['ego_features']
        if self.feature_scaler is not None:
            ego_features = self.feature_scaler.transform(ego_features)

        # ç¡®ä¿åºåˆ—é•¿åº¦ä¸€è‡´
        if len(ego_features) > self.sequence_length:
            ego_features = ego_features[:self.sequence_length]
        elif len(ego_features) < self.sequence_length:
            # å¡«å……
            padding = np.zeros((self.sequence_length - len(ego_features), ego_features.shape[1]))
            ego_features = np.vstack([ego_features, padding])

        # å¤„ç†é‚»å±…ç‰¹å¾
        neighbor_features = scene_data['neighbor_features']
        neighbor_tensor = torch.zeros(self.max_neighbors, self.sequence_length, len(self.temporal_features))

        for i, neigh_feat in enumerate(neighbor_features[:self.max_neighbors]):
            if neigh_feat.ndim == 1:
                neigh_feat = neigh_feat.reshape(1, -1)

            if self.feature_scaler is not None:
                neigh_feat = self.feature_scaler.transform(neigh_feat)

            if len(neigh_feat) > self.sequence_length:
                neigh_feat = neigh_feat[:self.sequence_length]
            elif len(neigh_feat) < self.sequence_length:
                padding = np.zeros((self.sequence_length - len(neigh_feat), neigh_feat.shape[1]))
                neigh_feat = np.vstack([neigh_feat, padding])

            neighbor_tensor[i] = torch.from_numpy(neigh_feat).float()

        # åˆ›å»ºç›®æ ‡ï¼ˆä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ä½ç½®ä½œä¸ºé¢„æµ‹ç›®æ ‡ï¼‰
        # æ–°çš„ç‰¹å¾é¡ºåº: [latitude(0), longitude(1), flight_level(2), vx(3), vy(4)]
        target_data = ego_features[-1, [0, 1]]  # [lat, lon]

        return {
            'scene_id': scene_data['scene_id'],
            'ego_features': torch.from_numpy(ego_features).float(),
            'neighbor_features': neighbor_tensor,
            'target': torch.from_numpy(target_data).float(),
            'layer': scene_data['layer']
        }


def create_social_patchtst_loaders(config_path: str = None, batch_size: int = 32,
                                  max_neighbors: int = 20, sequence_length: int = 600,
                                  num_workers: int = 4) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """
    åˆ›å»ºSocial-PatchTSTæ•°æ®åŠ è½½å™¨

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        batch_size: æ‰¹å¤§å°
        max_neighbors: æ¯ä¸ªåœºæ™¯æœ€å¤§é‚»å±…æ•°é‡
        sequence_length: åºåˆ—é•¿åº¦
        num_workers: æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°

    Returns:
        train_loader, val_loader, test_loader
    """
    # åŠ è½½é…ç½®æ–‡ä»¶
    if config_path is None:
        config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
                                 "config", "social_patchtst_config.yaml")

    config = load_config(config_path)

    # ä»é…ç½®æ–‡ä»¶è·å–æ•°æ®ç›®å½•
    scenes_dir = config.get('data.scenes_dir') or config.get('data.data_dir')
    if not scenes_dir:
        raise ValueError("é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ° data.scenes_dir æˆ– data.data_dir")

    scenes_path = Path(scenes_dir)

    # è·¯å¾„æ–‡ä»¶è·¯å¾„
    train_paths_file = scenes_path / "train_paths.txt"
    val_paths_file = scenes_path / "val_paths.txt"
    test_paths_file = scenes_path / "test_paths.txt"

    print("ğŸš€ åˆ›å»ºSocial-PatchTSTæ•°æ®åŠ è½½å™¨")
    print(f"   é…ç½®æ–‡ä»¶: {config_path}")
    print(f"   æ•°æ®ç›®å½•: {scenes_path}")
    print(f"   è®­ç»ƒè·¯å¾„: {train_paths_file}")
    print(f"   éªŒè¯è·¯å¾„: {val_paths_file}")
    print(f"   æµ‹è¯•è·¯å¾„: {test_paths_file}")

    # æ£€æŸ¥è·¯å¾„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not train_paths_file.exists():
        raise FileNotFoundError(f"è®­ç»ƒè·¯å¾„æ–‡ä»¶ä¸å­˜åœ¨: {train_paths_file}")
    if not val_paths_file.exists():
        raise FileNotFoundError(f"éªŒè¯è·¯å¾„æ–‡ä»¶ä¸å­˜åœ¨: {val_paths_file}")
    if not test_paths_file.exists():
        raise FileNotFoundError(f"æµ‹è¯•è·¯å¾„æ–‡ä»¶ä¸å­˜åœ¨: {test_paths_file}")

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = SocialPatchTSTDataset(str(scenes_path), max_neighbors, sequence_length, str(train_paths_file))
    val_dataset = SocialPatchTSTDataset(str(scenes_path), max_neighbors, sequence_length, str(val_paths_file))
    test_dataset = SocialPatchTSTDataset(str(scenes_path), max_neighbors, sequence_length, str(test_paths_file))

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        drop_last=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers
    )

    print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ!")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬ ({len(train_loader)} batches)")
    print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬ ({len(val_loader)} batches)")
    print(f"   æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬ ({len(test_loader)} batches)")
    print(f"   ç‰¹å¾ç»´åº¦: {len(CSV_FEATURE_COLUMNS['temporal_features'])}")

    return train_loader, val_loader, test_loader


def get_feature_info():
    """è·å–ç‰¹å¾ä¿¡æ¯"""
    return {
        'temporal_features': CSV_FEATURE_COLUMNS['temporal_features'],
        'spatial_features': CSV_FEATURE_COLUMNS['spatial_features'],
        'target_features': CSV_FEATURE_COLUMNS['target_features'],
        'n_temporal_features': len(CSV_FEATURE_COLUMNS['temporal_features']),
        'n_target_features': len(CSV_FEATURE_COLUMNS['target_features'])
    }