#!/usr/bin/env python3
"""
V9 åœºæ™¯åˆ†å±‚é‡‡æ ·è„šæœ¬
ä»å®Œæ•´çš„åœºæ™¯æ•°æ®é›†ä¸­ç§‘å­¦é‡‡æ ·ï¼Œç¡®ä¿æ•°æ®å¤šæ ·æ€§
"""

import os
import json
import argparse
import random
import shutil
from pathlib import Path
from typing import List, Dict, Tuple
import tqdm


def load_scene_metadata(scenes_dir: str) -> List[Dict]:
    """
    åŠ è½½æ‰€æœ‰åœºæ™¯çš„å…ƒæ•°æ®

    Args:
        scenes_dir: åœºæ™¯ç›®å½•è·¯å¾„

    Returns:
        åœºæ™¯å…ƒæ•°æ®åˆ—è¡¨
    """
    scenes = []
    scene_dirs = [d for d in os.listdir(scenes_dir)
                  if os.path.isdir(os.path.join(scenes_dir, d))]

    for scene_dir in scene_dirs:
        scene_path = os.path.join(scenes_dir, scene_dir)
        metadata_path = os.path.join(scene_path, 'metadata.json')
        if os.path.exists(metadata_path):
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
                metadata['scene_dir'] = scene_path
                scenes.append(metadata)

    return scenes


def stratified_sampling(scenes: List[Dict],
                         total_target: int = 250000,
                         solo_ratio: float = 0.4,
                         low_risk_ratio: float = 0.3,
                         high_risk_ratio: float = 0.3) -> Dict[str, List[str]]:
    """
    åˆ†å±‚é‡‡æ ·ç­–ç•¥

    Args:
        scenes: æ‰€æœ‰åœºæ™¯å…ƒæ•°æ®
        total_target: ç›®æ ‡æ€»åœºæ™¯æ•°
        solo_ratio: ç‹¬è‡ªé£è¡Œåœºæ™¯æ¯”ä¾‹
        low_risk_ratio: ä½é£é™©åœºæ™¯æ¯”ä¾‹
        high_risk_ratio: é«˜é£é™©åœºæ™¯æ¯”ä¾‹

    Returns:
        åˆ†å±‚é‡‡æ ·ç»“æœ
    """
    # åˆ†ç±»åœºæ™¯
    solo_scenes = []      # mindist = 9999 (ç‹¬è‡ªé£è¡Œ)
    low_risk_scenes = []   # 30-50 NM
    high_risk_scenes = []  # < 30 NM

    print("åˆ†ï¿½ï¿½åœºæ™¯...")
    for scene in tqdm.tqdm(scenes, desc="åˆ†æåœºæ™¯"):
        mindist = scene['mindist_nm']

        if mindist == 9999.0:
            solo_scenes.append(scene)
        elif mindist < 30.0:
            high_risk_scenes.append(scene)
        else:
            low_risk_scenes.append(scene)

    print(f"åœºæ™¯åˆ†ç±»å®Œæˆ:")
    print(f"  ç‹¬è‡ªé£è¡Œåœºæ™¯: {len(solo_scenes):,} ({len(solo_scenes)/len(scenes)*100:.1f}%)")
    print(f"  ä½é£é™©åœºæ™¯ (30-50NM): {len(low_risk_scenes):,} ({len(low_risk_scenes)/len(scenes)*100:.1f}%)")
    print(f"  é«˜é£é™©åœºæ™¯ (<30NM): {len(high_risk_scenes):,} ({len(high_risk_scenes)/len(scenes)*100:.1f}%)")

    # è®¡ç®—å„ç±»åˆ«ç›®æ ‡æ•°é‡
    solo_target = int(total_target * solo_ratio)
    low_risk_target = int(total_target * low_risk_ratio)
    high_risk_target = int(total_target * high_risk_ratio)

    print(f"\né‡‡æ ·ç›®æ ‡ (æ€»è®¡ {total_target:,}):")
    print(f"  è½¨è¿¹é¢„æµ‹åŸºç¡€ (ç‹¬è‡ªé£è¡Œ): {solo_target:,}")
    print(f"  ä½é£é™©äº¤äº’ (30-50NM): {low_risk_target:,}")
    print(f"  é«˜é£é™©äº¤äº’ (<30NM): {high_risk_target:,}")

    # æ‰§è¡Œé‡‡æ ·
    result = {}

    # é‡‡æ ·ç‹¬è‡ªé£è¡Œåœºæ™¯
    if len(solo_scenes) >= solo_target:
        result['solo'] = random.sample(solo_scenes, solo_target)
        print(f"  âœ“ éšæœºé‡‡æ · {solo_target:,} ä¸ªç‹¬è‡ªé£è¡Œåœºæ™¯")
    else:
        result['solo'] = solo_scenes  # å…¨éƒ¨ä½¿ç”¨
        print(f"  âš ï¸  ç‹¬è‡ªé£è¡Œåœºæ™¯ä¸è¶³ï¼Œä½¿ç”¨å…¨éƒ¨ {len(solo_scenes):,} ä¸ª")

    # é‡‡æ ·ä½é£é™©åœºæ™¯
    if len(low_risk_scenes) >= low_risk_target:
        result['low_risk'] = random.sample(low_risk_scenes, low_risk_target)
        print(f"  âœ“ éšæœºé‡‡æ · {low_risk_target:,} ä¸ªä½é£é™©åœºæ™¯")
    else:
        result['low_risk'] = low_risk_scenes
        print(f"  âš ï¸  ä½é£é™©åœºæ™¯ä¸è¶³ï¼Œä½¿ç”¨å…¨éƒ¨ {len(low_risk_scenes):,} ä¸ª")

    # é‡‡æ ·é«˜é£é™©åœºæ™¯
    if len(high_risk_scenes) >= high_risk_target:
        result['high_risk'] = random.sample(high_risk_scenes, high_risk_target)
        print(f"  âœ“ éšæœºé‡‡æ · {high_risk_target:,} ä¸ªé«˜é£é™©åœºæ™¯")
    else:
        result['high_risk'] = high_risk_scenes
        print(f"  âš ï¸  é«˜é£é™©åœºæ™¯ä¸è¶³ï¼Œä½¿ç”¨å…¨éƒ¨ {len(high_risk_scenes):,} ä¸ª")

    total_selected = sum(len(v) for v in result.values())
    print(f"\nå®é™…é‡‡æ ·æ€»æ•°: {total_selected:,}")

    return result


def create_sampled_dataset(sampled_scenes: Dict[str, List[Dict]],
                          output_dir: str) -> str:
    """
    åˆ›å»ºé‡‡æ ·åçš„æ•°æ®é›†ç›®å½•

    Args:
        sampled_scenes: åˆ†å±‚é‡‡æ ·ç»“æœ
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        è¾“å‡ºç›®å½•è·¯å¾„
    """
    output_path = os.path.join(output_dir, "scenes_sampled_250k")
    os.makedirs(output_path, exist_ok=True)

    print(f"\nåˆ›å»ºé‡‡æ ·æ•°æ®é›†: {output_path}")

    # å¤åˆ¶åœºæ™¯
    total_copied = 0
    for category, scenes in sampled_scenes.items():
        print(f"å¤åˆ¶{category}åœºæ™¯...")

        for scene in tqdm.tqdm(scenes, desc=f"å¤åˆ¶{category}"):
            scene_dir = scene['scene_dir']
            scene_id = os.path.basename(scene_dir)
            new_scene_dir = os.path.join(output_path, scene_id)

            # å¤åˆ¶æ•´ä¸ªåœºæ™¯ç›®å½•
            shutil.copytree(scene_dir, new_scene_dir, dirs_exist_ok=True)
            total_copied += 1

    print(f"\nâœ… æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
    print(f"è¾“å‡ºè·¯å¾„: {output_path}")
    print(f"åŒ…å«åœºæ™¯æ•°: {total_copied:,}")

    return output_path


def analyze_sampling_quality(sampled_dir: str):
    """
    åˆ†æé‡‡æ ·æ•°æ®é›†çš„è´¨é‡

    Args:
        sampled_dir: é‡‡æ ·æ•°æ®é›†ç›®å½•
    """
    print(f"\n=== é‡‡æ ·æ•°æ®é›†è´¨é‡åˆ†æ ===")

    # åŠ è½½é‡‡æ ·åçš„åœºæ™¯å…ƒæ•°æ®
    sampled_scenes = load_scene_metadata(sampled_dir)

    # ç»Ÿè®¡mindiståˆ†å¸ƒ
    mindist_values = [scene['mindist_nm'] for scene in sampled_scenes]

    mindist_stats = {
        'min': min(mindist_values),
        'max': max(mindist_values),
        'mean': sum(mindist_values) / len(mindist_values),
        'median': sorted(mindist_values)[len(mindist_values) // 2]
    }

    print(f"Mindistç»Ÿè®¡ (æ€»åœºæ™¯æ•°: {len(sampled_scenes):,}):")
    print(f"  æœ€å°è·ç¦»: {mindist_stats['min']:.1f} NM")
    print(f"  æœ€å¤§è·ç¦»: {mindist_stats['max']:.1f} NM")
    print(f"  å¹³å‡è·ç¦»: {mindist_stats['mean']:.1f} NM")
    print(f"  ä¸­ä½æ•°è·ç¦»: {mindist_stats['median']:.1f} NM")

    # ç»Ÿè®¡é‚»å±…æ•°é‡åˆ†å¸ƒ
    neighbor_counts = [scene['n_neighbors'] for scene in sampled_scenes]
    neighbor_stats = {
        'mean': sum(neighbor_counts) / len(neighbor_counts),
        'max': max(neighbor_counts),
        'zero_count': sum(1 for n in neighbor_counts if n == 0)
    }

    print(f"\né‚»å±…æ•°é‡ç»Ÿè®¡:")
    print(f"  å¹³å‡é‚»å±…æ•°: {neighbor_stats['mean']:.1f}")
    print(f"  æœ€å¤§é‚»å±…æ•°: {neighbor_stats['max']}")
    print(f"  æ— é‚»å±…åœºæ™¯: {neighbor_stats['zero_count']} ({neighbor_stats['zero_count']/len(sampled_scenes)*100:.1f}%)")

    return mindist_stats, neighbor_stats


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='V9åœºæ™¯åˆ†å±‚é‡‡æ ·è„šæœ¬')
    parser.add_argument('--scenes-dir', type=str,
                       default='/mnt/d/model/adsb_scenes/scenes',
                       help='åŸå§‹åœºæ™¯æ•°æ®ç›®å½•')
    parser.add_argument('--output-dir', type=str,
                       default='/mnt/d/model/adsb_scenes',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--total-target', type=int, default=250000,
                       help='ç›®æ ‡æ€»åœºæ™¯æ•°')
    parser.add_argument('--solo-ratio', type=float, default=0.4,
                       help='ç‹¬è‡ªé£è¡Œåœºæ™¯æ¯”ä¾‹ (0.0-1.0)')
    parser.add_argument('--low-risk-ratio', type=float, default=0.3,
                       help='ä½é£é™©åœºæ™¯æ¯”ä¾‹ (0.0-1.0)')
    parser.add_argument('--high-risk-ratio', type=float, default=0.3,
                       help='é«˜é£é™©åœºæ™¯æ¯”ä¾‹ (0.0-1.0)')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--analyze-only', action='store_true',
                       help='åªåˆ†æç°æœ‰æ•°æ®ï¼Œä¸è¿›è¡Œé‡‡æ ·')

    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)

    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(args.scenes_dir):
        print(f"é”™è¯¯ï¼šåœºæ™¯ç›®å½•ä¸å­˜åœ¨: {args.scenes_dir}")
        return

    print("=== V9 åœºæ™¯åˆ†å±‚é‡‡æ · ===")
    print(f"è¾“å…¥ç›®å½•: {args.scenes_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ç›®æ ‡åœºæ™¯æ•°: {args.total_target:,}")
    print(f"é‡‡æ ·æ¯”ä¾‹: ç‹¬è‡ªé£è¡Œ={args.solo_ratio:.1f}, ä½é£é™©={args.low_risk_ratio:.1f}, é«˜é£é™©={args.high_risk_ratio:.1f}")

    # åŠ è½½æ‰€æœ‰åœºæ™¯å…ƒæ•°æ®
    print("\nåŠ è½½åœºæ™¯å…ƒæ•°æ®...")
    all_scenes = load_scene_metadata(args.scenes_dir)

    if not all_scenes:
        print("é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åœºæ™¯æ•°æ®")
        return

    print(f"æ‰¾åˆ° {len(all_scenes):,} ä¸ªåœºæ™¯")

    if args.analyze_only:
        # åªåˆ†æç°æœ‰æ•°æ®
        print("\nè·³è¿‡é‡‡æ ·ï¼Œä»…åˆ†æç°æœ‰æ•°æ®...")
        mindist_stats, neighbor_stats = analyze_sampling_quality(args.scenes_dir)
        return

    # æ‰§è¡Œåˆ†å±‚é‡‡æ ·
    print(f"\nå¼€å§‹åˆ†å±‚é‡‡æ ·...")
    sampled_scenes = stratified_sampling(
        all_scenes,
        total_target=args.total_target,
        solo_ratio=args.solo_ratio,
        low_risk_ratio=args.low_risk_ratio,
        high_risk_ratio=args.high_risk_ratio
    )

    # åˆ›å»ºé‡‡æ ·æ•°æ®é›†
    output_path = create_sampled_dataset(sampled_scenes, args.output_dir)

    # åˆ†æé‡‡æ ·è´¨é‡
    mindist_stats, neighbor_stats = analyze_sampling_quality(output_path)

    print("\nğŸ¯ V9 åˆ†å±‚é‡‡æ ·å®Œæˆï¼")
    print("ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿æ¨¡å‹æ—¢èƒ½é¢„æµ‹æ­£å¸¸è½¨è¿¹ï¼Œåˆèƒ½å¤„ç†ç´§æ€¥é¿è®©ã€‚")


if __name__ == "__main__":
    main()