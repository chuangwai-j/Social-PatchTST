"""
è®­ç»ƒè„šæœ¬
è®­ç»ƒSocial-PatchTSTæ¨¡å‹
"""

import os
import sys
import argparse
import logging
import time
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model import SocialPatchTST, create_model
from data.dataset.scene_dataset import create_social_patchtst_loaders
from config.config_manager import load_config


class Trainer:
    """
    è®­ç»ƒå™¨ç±»
    """

    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = load_config(config_path)
        self.setup_logging()
        self.device = self._setup_device()

        # åˆ›å»ºæ¨¡å‹
        self.model = create_model(config_path)
        self.model.to(self.device)

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        model_info = self.model.get_model_info()
        self.logger.info(f"æ¨¡å‹ä¿¡æ¯: {model_info}")

        # åˆ›å»ºåœºæ™¯æ•°æ®åŠ è½½å™¨
        scenes_dir = self.config.get('data.scenes_dir', '/tmp/test_scenes')

        # æ£€æŸ¥åœºæ™¯ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(scenes_dir):
            self.logger.error(f"åœºæ™¯æ•°æ®ç›®å½•ä¸å­˜åœ¨: {scenes_dir}")
            self.logger.error("è¯·å…ˆè¿è¡Œåœºæ™¯æ•°æ®ç”Ÿæˆå™¨:")
            self.logger.error(f"python data/dataset/data_processor.py --input-dir /mnt/d/adsb --output-dir {os.path.dirname(scenes_dir)}")
            raise FileNotFoundError(f"åœºæ™¯æ•°æ®ç›®å½•ä¸å­˜åœ¨: {scenes_dir}")

        try:
            self.train_loader, self.val_loader, self.test_loader = create_social_patchtst_loaders(
                config_path=config_path,
                batch_size=self.config.get('training.batch_size', 4),
                max_neighbors=self.config.get('social_transformer.max_aircrafts', 50),
                sequence_length=self.config.get('data.history_length', 600),
                num_workers=self.config.get('device.num_workers', 4)
            )
        except Exception as e:
            self.logger.error(f"åˆ›å»ºæ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
            self.logger.error("è¯·ç¡®ä¿åœºæ™¯æ•°æ®å·²æ­£ç¡®ç”Ÿæˆ")
            raise

        # è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.setup_optimizer_scheduler()

        # è®¾ç½®æŸå¤±å‡½æ•°
        self.criterion = self.model.compute_loss

        # è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = self.config.get('device.mixed_precision', True)
        if self.use_amp:
            self.scaler = torch.cuda.amp.GradScaler()

        # è®¾ç½®TensorBoard
        self.setup_tensorboard()

        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.best_val_loss = float('inf')
        self.train_losses = []
        self.val_losses = []

    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
        gpu_ids = self.config.get('device.gpu_ids', [0])

        if torch.cuda.is_available() and gpu_ids:
            device = torch.device(f'cuda:{gpu_ids[0]}')
            self.logger.info(f"ä½¿ç”¨GPU: {gpu_ids}")

            # å¤šGPUè®­ç»ƒ
            if len(gpu_ids) > 1:
                self.logger.info(f"ä½¿ç”¨å¤šGPUè®­ç»ƒ: {gpu_ids}")
        else:
            device = torch.device('cpu')
            self.logger.info("ä½¿ç”¨CPUè®­ç»ƒ")

        return device

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_config = self.config.get('logging', {})
        log_dir = log_config.get('log_dir', './logs')
        os.makedirs(log_dir, exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(log_dir, 'training.log')),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def setup_optimizer_scheduler(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        training_config = self.config.training_config

        # ä¼˜åŒ–å™¨
        optimizer_name = training_config.get('optimizer', 'AdamW')
        learning_rate = training_config.get('learning_rate', 0.0001)
        weight_decay = training_config.get('weight_decay', 0.01)

        if optimizer_name == 'AdamW':
            self.optimizer = optim.AdamW(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
        elif optimizer_name == 'Adam':
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_name}")

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_config = training_config.get('scheduler', {})
        scheduler_name = scheduler_config.get('name', 'CosineAnnealingLR')

        if scheduler_name == 'CosineAnnealingLR':
            T_max = scheduler_config.get('T_max', 100)
            eta_min = scheduler_config.get('eta_min', 0.00001)
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, T_max=T_max, eta_min=eta_min
            )
        elif scheduler_name == 'StepLR':
            step_size = scheduler_config.get('step_size', 30)
            gamma = scheduler_config.get('gamma', 0.1)
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer, step_size=step_size, gamma=gamma
            )
        else:
            self.scheduler = None

    def setup_tensorboard(self):
        """è®¾ç½®TensorBoard"""
        log_config = self.config.get('logging', {})
        tensorboard_dir = log_config.get('tensorboard_dir', './logs/tensorboard')
        os.makedirs(tensorboard_dir, exist_ok=True)

        self.writer = SummaryWriter(tensorboard_dir)

    def train_epoch(self) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0

        pbar = tqdm(self.train_loader, desc=f"Epoch {self.epoch+1} Training")

        for batch_idx, batch in enumerate(pbar):
            # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                    for k, v in batch.items()}

            # å‰å‘ä¼ æ’­
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    output = self.model(batch, teacher_forcing_ratio=0.5)
                    losses = self.criterion(
                        output['predictions'], batch['targets'], batch.get('distance_matrix', None)
                    )
            else:
                output = self.model(batch, teacher_forcing_ratio=0.5)
                losses = self.criterion(
                    output['predictions'], batch['targets'], batch.get('distance_matrix', None)
                )

            # é˜²ç©ºæ´ï¼šç©ºæ ·æœ¬lossç½®0
            if batch['targets'].numel() == 0:
                losses = {'total_loss': 0.0 * output['predictions'].sum()}

            # åº”ç”¨æ ·æœ¬æƒé‡
            if 'sample_weight' in batch:
                sample_weights = batch['sample_weight'].to(self.device)
                # å¯¹æ¯ä¸ªæŸå¤±é¡¹åº”ç”¨æ ·æœ¬æƒé‡
                for loss_name in losses:
                    if loss_name != 'total_loss':  # total_lossä¼šåœ¨compute_lossä¸­é‡æ–°è®¡ç®—
                        losses[loss_name] = losses[loss_name] * sample_weights.mean()

                # é‡æ–°è®¡ç®—åŠ æƒçš„æ€»æŸå¤±
                loss_weights = self.config.get('training.loss_weights', {})
                position_weight = loss_weights.get('position', 1.0)
                altitude_weight = loss_weights.get('altitude', 1.0)
                velocity_weight = loss_weights.get('velocity', 0.5)
                mindist_weight = loss_weights.get('mindist', 2.0)

                losses['total_loss'] = (
                    position_weight * losses['position_loss'] +
                    altitude_weight * losses['altitude_loss'] +
                    velocity_weight * losses['velocity_loss'] +
                    mindist_weight * losses['mindist_loss']
                )

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()

            if self.use_amp:
                self.scaler.scale(losses['total_loss']).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                losses['total_loss'].backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()

            # è®°å½•æŸå¤±
            total_loss += losses['total_loss'].item()
            num_batches += 1

            # NaN/Infæ£€æµ‹
            losses['total_loss'] = torch.nan_to_num(losses['total_loss'], nan=0.0, posinf=1.0, neginf=1e-6)

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f"{losses['total_loss'].item():.6f}",
                'Pos': f"{losses['position_loss'].item():.6f}",
                'Alt': f"{losses['altitude_loss'].item():.6f}",
                'Vel': f"{losses['velocity_loss'].item():.6f}",
                'MD': f"{losses['mindist_loss'].item():.6f}"
            })

            # è®°å½•åˆ°TensorBoard
            global_step = self.epoch * len(self.train_loader) + batch_idx
            for loss_name, loss_value in losses.items():
                self.writer.add_scalar(f'Train/{loss_name}', loss_value.item(), global_step)

        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        self.train_losses.append(avg_loss)

        return avg_loss

    def validate_epoch(self) -> float:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc=f"Epoch {self.epoch+1} Validation")

            for batch_idx, batch in enumerate(pbar):
                # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                        for k, v in batch.items()}

                # å‰å‘ä¼ æ’­
                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        output = self.model(batch, teacher_forcing_ratio=0.0)
                        losses = self.criterion(
                            output['predictions'], batch['targets'], batch.get('distance_matrix', None)
                        )
                else:
                    output = self.model(batch, teacher_forcing_ratio=0.0)
                    losses = self.criterion(
                        output['predictions'], batch['targets'], batch.get('distance_matrix', None)
                    )

                # é˜²ç©ºæ´ï¼šç©ºæ ·æœ¬lossç½®0
                if batch['targets'].numel() == 0:
                    losses = {'total_loss': 0.0 * output['predictions'].sum()}

                # è®°å½•æŸå¤±
                total_loss += losses['total_loss'].item()
                num_batches += 1

                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'Loss': f"{losses['total_loss'].item():.6f}",
                    'Pos': f"{losses['position_loss'].item():.6f}",
                    'Alt': f"{losses['altitude_loss'].item():.6f}",
                    'Vel': f"{losses['velocity_loss'].item():.6f}",
                    'MD': f"{losses['mindist_loss'].item():.6f}"
                })

        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        self.val_losses.append(avg_loss)

        # è®°å½•åˆ°TensorBoard
        self.writer.add_scalar('Validation/Total_Loss', avg_loss, self.epoch)

        return avg_loss

    def save_checkpoint(self, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'best_val_loss': self.best_val_loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'config': self.config.config
        }

        # åˆ›å»ºcheckpointsç›®å½•
        checkpoints_dir = './checkpoints'
        os.makedirs(checkpoints_dir, exist_ok=True)

        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(checkpoints_dir, 'latest_checkpoint.pth')
        torch.save(checkpoint, checkpoint_path)

        # ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°checkpoints/
        if is_best:
            best_path = os.path.join(checkpoints_dir, 'best_model.pth')
            torch.save(checkpoint, best_path)
            self.logger.info(f"ğŸ’ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {best_path}")

        # å®šæœŸä¿å­˜
        save_freq = self.config.get('logging.save_freq', 10)
        if (self.epoch + 1) % save_freq == 0:
            epoch_path = os.path.join(
                checkpoints_dir, f'checkpoint_epoch_{self.epoch+1}.pth'
            )
            torch.save(checkpoint, epoch_path)

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        training_config = self.config.get('training', {})
        epochs = training_config.get('epochs', 100)
        patience = training_config.get('patience', 10)

        self.logger.info(f"å¼€å§‹è®­ç»ƒï¼Œæ€»epochs: {epochs}")

        # æ—©åœè®¡æ•°å™¨
        patience_counter = 0

        for epoch in range(epochs):
            self.epoch = epoch

            # è®­ç»ƒ
            train_loss = self.train_epoch()
            self.logger.info(f"Epoch {epoch+1} - Train Loss: {train_loss:.6f}")

            # éªŒè¯
            val_loss = self.validate_epoch()
            self.logger.info(f"Epoch {epoch+1} - Val Loss: {val_loss:.6f}")

            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step()
                current_lr = self.optimizer.param_groups[0]['lr']
                self.writer.add_scalar('Learning_Rate', current_lr, epoch)
                self.logger.info(f"Learning Rate: {current_lr:.8f}")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = val_loss
                patience_counter = 0
                self.logger.info(f"æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {val_loss:.6f}")
            else:
                patience_counter += 1

            self.save_checkpoint(is_best)

            # æ—©åœ
            if patience_counter >= patience:
                self.logger.info(f"æ—©åœè§¦å‘ï¼Œpatience: {patience}")
                break

        self.logger.info("è®­ç»ƒå®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒSocial-PatchTSTæ¨¡å‹')
    parser.add_argument('--config', type=str,
                       default='config/social_patchtst_config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', action='store_true',
                       help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    parser.add_argument('--test', action='store_true',
                       help='è¿è¡Œæµ‹è¯•æ¨¡å¼è€Œä¸æ˜¯è®­ç»ƒ')
    parser.add_argument('--scenes_dir', type=str,
                       help='åœºæ™¯æ•°æ®ç›®å½•è·¯å¾„')

    args = parser.parse_args()

    if not os.path.exists(args.config):
        print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return

    # å¦‚æœæŒ‡å®šäº†æµ‹è¯•æ¨¡å¼
    if args.test:
        from data.dataset import SceneDataset
        from config.config_manager import load_config

        config = load_config(args.config)
        scenes_dir = args.scenes_dir or config.data_config.get('scenes_dir', '/tmp/test_scenes')

        print(f"æµ‹è¯•åœºæ™¯æ•°æ®åŠ è½½...")
        print(f"åœºæ™¯ç›®å½•: {scenes_dir}")

        dataset = SceneDataset(
            scenes_data=scenes_dir,
            config_path=args.config,
            max_neighbors=10
        )

        print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")

        if len(dataset) > 0:
            sample = dataset[0]
            print(f"æ ·æœ¬å½¢çŠ¶:")
            print(f"  - æ—¶åºæ•°æ®: {sample['temporal'].shape}")
            print(f"  - ç©ºé—´æ•°æ®: {sample['spatial'].shape}")
            print(f"  - ç›®æ ‡æ•°æ®: {sample['targets'].shape}")
            print(f"  - è·ç¦»çŸ©é˜µ: {sample['distance_matrix'].shape}")

        print("âœ… æµ‹è¯•å®Œæˆ!")
        return

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(args.config)

    # å¦‚æœæŒ‡å®šäº†æ¢å¤è®­ç»ƒï¼ŒåŠ è½½æ£€æŸ¥ç‚¹
    if args.resume:
        checkpoint_path = './checkpoints/latest_checkpoint.pth'
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path, map_location=trainer.device)
            trainer.model.load_state_dict(checkpoint['model_state_dict'])
            trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if trainer.scheduler and checkpoint['scheduler_state_dict']:
                trainer.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            trainer.epoch = checkpoint['epoch']
            trainer.best_val_loss = checkpoint['best_val_loss']
            trainer.train_losses = checkpoint['train_losses']
            trainer.val_losses = checkpoint['val_losses']
            trainer.logger.info(f"ä»epoch {trainer.epoch}æ¢å¤è®­ç»ƒï¼Œæœ€ä½³éªŒè¯æŸå¤±: {trainer.best_val_loss:.6f}")
        else:
            trainer.logger.warning("æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")

    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == "__main__":
    main()