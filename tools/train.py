"""
è®­ç»ƒè„šæœ¬
è®­ç»ƒSocial-PatchTSTæ¨¡å‹
"""

import os
import sys
import argparse
import logging
import time
import yaml
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error
from typing import Dict, List, Tuple, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model import SocialPatchTST, create_model
from data.dataset.scene_dataset import create_social_patchtst_loaders
from config.config_manager import load_config


# -----------------------------------------------------------------
# è®ºæ–‡æ€§èƒ½æŒ‡æ ‡è®¡ç®—å‡½æ•° (Kimiçš„å»ºè®®)
# -----------------------------------------------------------------

def calculate_rmse(pred, truth, feature_indices):
    """è®¡ç®—æŒ‡å®šç‰¹å¾ç»´åº¦çš„RMSE"""
    if pred.ndim < 3 or truth.ndim < 3 or not feature_indices:
        return 0.0
    errors = pred[:, :, feature_indices] - truth[:, :, feature_indices]
    return np.sqrt(np.mean(errors**2))

def calculate_mae(pred, truth, feature_indices):
    """è®¡ç®—æŒ‡å®šç‰¹å¾ç»´åº¦çš„MAE"""
    if pred.ndim < 3 or truth.ndim < 3 or not feature_indices:
        return 0.0
    errors = pred[:, :, feature_indices] - truth[:, :, feature_indices]
    return np.mean(np.abs(errors))

def calculate_far(pred, truth, cpa_threshold_nm=3.0, alt_threshold_ft=500):
    """
    è®¡ç®—è™šè­¦ç‡ (False Alarm Rate) - ç¤¾äº¤æ¨¡å‹ç‰¹æœ‰æŒ‡æ ‡

    Args:
        pred: é¢„æµ‹ç»“æœ [N, T_out, features]
        truth: çœŸå®æ ‡ç­¾ [N, T_out, features]
        cpa_threshold_nm: CPAé˜ˆå€¼ (æµ·é‡Œ)
        alt_threshold_ft: é«˜åº¦é˜ˆå€¼ (è‹±å°º)

    Returns:
        far: è™šè­¦ç‡ (0-1ä¹‹é—´çš„å€¼)
    """
    # æ ¹æ®æ‚¨çš„æ•°æ®æ ¼å¼ï¼Œå‡è®¾ç‰¹å¾é¡ºåºä¸º [lat, lon, alt, vx, vy]
    # é¢„æµ‹çš„å†²çªæ•°é‡ï¼šé¢„æµ‹CPA < é˜ˆå€¼ æˆ– é«˜åº¦å·® > é˜ˆå€¼
    pred_conflicts = 0

    # çœŸå®çš„å†²çªæ•°é‡ï¼šå®é™…CPA < é˜ˆå€¼ æˆ– é«˜åº¦å·® > é˜ˆå€¼
    truth_conflicts = 0

    # ç®€åŒ–å®ç°ï¼šåŸºäºé«˜åº¦å·®å’Œä½ç½®è·ç¦»çš„ç®€åŒ–å†²çªæ£€æµ‹
    for i in range(pred.shape[0]):  # batch
        for t in range(pred.shape[1]):  # time
            # è®¡ç®—ä½ç½®è·ç¦» (ç®€åŒ–ä¸ºæ¬§å‡ é‡Œå¾—è·ç¦»)
            pos_dist = np.sqrt(pred[i, t, 0]**2 + pred[i, t, 1]**2)

            # è®¡ç®—é«˜åº¦å·®
            alt_diff = abs(pred[i, t, 2] - truth[i, t, 2]) if pred.shape[2] > 2 else 0

            # é¢„æµ‹å†²çªåˆ¤æ–­
            if pos_dist < cpa_threshold_nm or alt_diff > alt_threshold_ft:
                pred_conflicts += 1

            # çœŸå®å†²çªåˆ¤æ–­
            truth_pos_dist = np.sqrt(truth[i, t, 0]**2 + truth[i, t, 1]**2)
            truth_alt_diff = abs(truth[i, t, 2] - truth[i, t, 2]) if truth.shape[2] > 2 else 0
            if truth_pos_dist < cpa_threshold_nm or truth_alt_diff > alt_threshold_ft:
                truth_conflicts += 1

    total_predictions = pred.shape[0] * pred.shape[1]
    total_safe = total_predictions - truth_conflicts

    if total_safe == 0:
        return 0.0

    # FAR = é¢„æµ‹å†²çªä½†å®é™…å®‰å…¨çš„æ•°é‡ / å®é™…å®‰å…¨çš„æ€»æ•°
    false_alarms = max(0, pred_conflicts - truth_conflicts)
    return false_alarms / total_safe


class Trainer:
    """
    è®­ç»ƒå™¨ç±»
    """

    def __init__(self, config_path: str, is_baseline: bool = False):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            is_baseline: æ˜¯å¦è¿è¡ŒBaselineæ¨¡å¼ï¼ˆå…³é—­ç¤¾äº¤æ¨¡å—ï¼‰
        """
        self.config = load_config(config_path)
        self.is_baseline = is_baseline
        self.setup_logging()
        self.device = self._setup_device()

        # åˆ›å»ºæ¨¡å‹
        self.model = create_model(config_path, is_baseline=self.is_baseline)
        mode_name = "Baseline (åŸç‰ˆPatchTST)" if self.is_baseline else "Social-PatchTST"
        self.logger.info(f"è¿è¡Œæ¨¡å¼: {mode_name}")
        self.model.to(self.device)

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        model_info = self.model.get_model_info()
        self.logger.info(f"æ¨¡å‹ä¿¡æ¯: {model_info}")

        # åˆ›å»ºåœºæ™¯æ•°æ®åŠ è½½å™¨
        scenes_dir = self.config.get('data.scenes_dir', '/tmp/test_scenes')

        # æ£€æŸ¥åœºæ™¯ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(scenes_dir):
            self.logger.error(f"åœºæ™¯æ•°æ®ç›®å½•ä¸å­˜åœ¨: {scenes_dir}")
            self.logger.error("è¯·å…ˆè¿è¡Œåœºæ™¯æ•°æ®ç”Ÿæˆå™¨:")
            self.logger.error(f"python data/dataset/data_processor.py --input-dir /mnt/d/adsb --output-dir {os.path.dirname(scenes_dir)}")
            raise FileNotFoundError(f"åœºæ™¯æ•°æ®ç›®å½•ä¸å­˜åœ¨: {scenes_dir}")

        try:
            self.train_loader, self.val_loader, self.test_loader = create_social_patchtst_loaders(
                config_path=config_path,
                batch_size=self.config.get('training.batch_size', 4),
                max_neighbors=self.config.get('social_transformer.max_aircrafts', 50),
                sequence_length=self.config.get('data.history_length', 600),
                num_workers=self.config.get('device.num_workers', 4)
            )
        except Exception as e:
            self.logger.error(f"åˆ›å»ºæ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
            self.logger.error("è¯·ç¡®ä¿åœºæ™¯æ•°æ®å·²æ­£ç¡®ç”Ÿæˆ")
            raise

        # è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.setup_optimizer_scheduler()

        # è®¾ç½®æŸå¤±å‡½æ•°
        self.criterion = self.model.compute_loss

        # è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = self.config.get('device.mixed_precision', True)
        if self.use_amp:
            self.scaler = torch.cuda.amp.GradScaler()

        # è®¾ç½®TensorBoard
        self.setup_tensorboard()

        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.best_val_loss = float('inf')

        # æŸå¤±å†å²è®°å½•
        self.train_losses = []
        self.val_losses = []

        # å…¨é¢çš„æŒ‡æ ‡å†å²è®°å½• (Geminiå»ºè®®)
        self.metrics_history = {
            # è®­ç»ƒæŸå¤±æŒ‡æ ‡
            'train_total_loss': [],
            'train_position_loss': [],
            'train_altitude_loss': [],
            'train_velocity_loss': [],
            'train_mindist_loss': [],

            # éªŒè¯æŸå¤±æŒ‡æ ‡
            'val_total_loss': [],
            'val_position_loss': [],
            'val_altitude_loss': [],
            'val_velocity_loss': [],
            'val_mindist_loss': [],

            # æ€§èƒ½æŒ‡æ ‡ (RMSE, MAE)
            'val_position_rmse': [],
            'val_altitude_rmse': [],
            'val_velocity_rmse': [],
            'val_position_mae': [],
            'val_altitude_mae': [],
            'val_velocity_mae': [],

            # ç¤¾äº¤æ¨¡å‹ç‰¹æœ‰æŒ‡æ ‡
            'val_far': [],
            'val_mindist_mean': [],

            # è®­ç»ƒä¿¡æ¯
            'learning_rates': [],
            'epoch_times': []
        }

    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
        gpu_ids = self.config.get('device.gpu_ids', [0])

        if torch.cuda.is_available() and gpu_ids:
            device = torch.device(f'cuda:{gpu_ids[0]}')
            self.logger.info(f"ä½¿ç”¨GPU: {gpu_ids}")

            # å¤šGPUè®­ç»ƒ
            if len(gpu_ids) > 1:
                self.logger.info(f"ä½¿ç”¨å¤šGPUè®­ç»ƒ: {gpu_ids}")
        else:
            device = torch.device('cpu')
            self.logger.info("ä½¿ç”¨CPUè®­ç»ƒ")

        return device

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_config = self.config.get('logging', {})
        log_dir = log_config.get('log_dir', './logs')
        os.makedirs(log_dir, exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(log_dir, 'training.log')),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def setup_optimizer_scheduler(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        training_config = self.config.training_config

        # ä¼˜åŒ–å™¨
        optimizer_name = training_config.get('optimizer', 'AdamW')
        learning_rate = training_config.get('learning_rate', 0.0001)
        weight_decay = training_config.get('weight_decay', 0.01)

        if optimizer_name == 'AdamW':
            self.optimizer = optim.AdamW(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
        elif optimizer_name == 'Adam':
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_name}")

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_config = training_config.get('scheduler', {})
        scheduler_name = scheduler_config.get('name', 'CosineAnnealingLR')

        if scheduler_name == 'CosineAnnealingLR':
            T_max = scheduler_config.get('T_max', 100)
            eta_min = scheduler_config.get('eta_min', 0.00001)
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, T_max=T_max, eta_min=eta_min
            )
        elif scheduler_name == 'StepLR':
            step_size = scheduler_config.get('step_size', 30)
            gamma = scheduler_config.get('gamma', 0.1)
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer, step_size=step_size, gamma=gamma
            )
        else:
            self.scheduler = None

    def setup_tensorboard(self):
        """è®¾ç½®TensorBoard"""
        log_config = self.config.get('logging', {})
        tensorboard_dir = log_config.get('tensorboard_dir', './logs/tensorboard')
        os.makedirs(tensorboard_dir, exist_ok=True)

        self.writer = SummaryWriter(tensorboard_dir)

    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepochï¼Œè¿”å›è¯¦ç»†çš„æŸå¤±å­—å…¸"""
        self.model.train()

        # ç´¯è®¡å„é¡¹æŸå¤±
        total_losses = {'total_loss': 0.0, 'position_loss': 0.0, 'altitude_loss': 0.0, 'velocity_loss': 0.0, 'mindist_loss': 0.0}
        num_batches = 0

        pbar = tqdm(self.train_loader, desc=f"Epoch {self.epoch+1} Training")

        for batch_idx, batch in enumerate(pbar):
            # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                    for k, v in batch.items()}

            # å‰å‘ä¼ æ’­
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    output = self.model(batch, teacher_forcing_ratio=0.5)
                    losses = self.criterion(
                        output['predictions'], batch['targets'], batch.get('distance_matrix', None)
                    )
            else:
                output = self.model(batch, teacher_forcing_ratio=0.5)
                losses = self.criterion(
                    output['predictions'],  # è¿™æ˜¯4ä¸ªé¢„æµ‹å¤´çš„å­—å…¸
                    batch['targets'],       # è¿™æ˜¯targetså¼ é‡
                    batch.get('distance_matrix', None)
                )

            # é˜²ç©ºæ´ï¼šç©ºæ ·æœ¬lossç½®0
            if batch['targets'].numel() == 0:
                losses = {'total_loss': 0.0 * output['predictions'].sum(), 'position_loss': 0.0 * output['predictions'].sum(),
                         'altitude_loss': 0.0 * output['predictions'].sum(), 'velocity_loss': 0.0 * output['predictions'].sum(),
                         'mindist_loss': 0.0 * output['predictions'].sum()}

            # åº”ç”¨æ ·æœ¬æƒé‡
            if 'sample_weight' in batch:
                sample_weights = batch['sample_weight'].to(self.device)
                # å¯¹æ¯ä¸ªæŸå¤±é¡¹åº”ç”¨æ ·æœ¬æƒé‡
                for loss_name in losses:
                    if loss_name != 'total_loss':  # total_lossä¼šåœ¨compute_lossä¸­é‡æ–°è®¡ç®—
                        losses[loss_name] = losses[loss_name] * sample_weights.mean()

                # é‡æ–°è®¡ç®—åŠ æƒçš„æ€»æŸå¤±
                loss_weights = self.config.get('training.loss_weights', {})
                position_weight = loss_weights.get('position', 1.0)
                altitude_weight = loss_weights.get('altitude', 1.0)
                velocity_weight = loss_weights.get('velocity', 0.5)
                mindist_weight = loss_weights.get('mindist', 2.0)

                losses['total_loss'] = (
                    position_weight * losses['position_loss'] +
                    altitude_weight * losses['altitude_loss'] +
                    velocity_weight * losses['velocity_loss'] +
                    mindist_weight * losses['mindist_loss']
                )

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()

            if self.use_amp:
                self.scaler.scale(losses['total_loss']).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                losses['total_loss'].backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()

            # ç´¯è®¡æŸå¤±
            for loss_name in total_losses:
                total_losses[loss_name] += losses[loss_name].item()
            num_batches += 1

            # NaN/Infæ£€æµ‹
            losses['total_loss'] = torch.nan_to_num(losses['total_loss'], nan=0.0, posinf=1.0, neginf=1e-6)

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f"{losses['total_loss'].item():.6f}",
                'Pos': f"{losses['position_loss'].item():.6f}",
                'Alt': f"{losses['altitude_loss'].item():.6f}",
                'Vel': f"{losses['velocity_loss'].item():.6f}",
                'MD': f"{losses['mindist_loss'].item():.6f}"
            })

            # è®°å½•åˆ°TensorBoard
            global_step = self.epoch * len(self.train_loader) + batch_idx
            for loss_name, loss_value in losses.items():
                self.writer.add_scalar(f'Train/{loss_name}', loss_value.item(), global_step)

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_losses = {loss_name: total_loss / num_batches if num_batches > 0 else 0.0
                     for loss_name, total_loss in total_losses.items()}

        # ä¿æŒå‘åå…¼å®¹æ€§
        self.train_losses.append(avg_losses['total_loss'])

        return avg_losses

    def validate_epoch(self) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepochï¼Œè¿”å›è¯¦ç»†çš„æŸå¤±å’Œæ€§èƒ½æŒ‡æ ‡"""
        self.model.eval()

        # ç´¯è®¡æŸå¤±å’Œæ€§èƒ½æŒ‡æ ‡
        total_losses = {'total_loss': 0.0, 'position_loss': 0.0, 'altitude_loss': 0.0, 'velocity_loss': 0.0, 'mindist_loss': 0.0}

        # æ€§èƒ½æŒ‡æ ‡ç´¯è®¡
        all_predictions = []
        all_targets = []
        all_mindist_predictions = []

        num_batches = 0

        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc=f"Epoch {self.epoch+1} Validation")

            for batch_idx, batch in enumerate(pbar):
                # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                        for k, v in batch.items()}

                # å‰å‘ä¼ æ’­
                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        output = self.model(batch, teacher_forcing_ratio=0.0)
                        losses = self.criterion(
                            output['predictions'], batch['targets'], batch.get('distance_matrix', None)
                        )
                else:
                    output = self.model(batch, teacher_forcing_ratio=0.0)
                    losses = self.criterion(
                        output['predictions'], batch['targets'], batch.get('distance_matrix', None)
                    )

                # é˜²ç©ºæ´ï¼šç©ºæ ·æœ¬lossç½®0
                if batch['targets'].numel() == 0:
                    losses = {'total_loss': 0.0 * output['predictions'].sum(), 'position_loss': 0.0 * output['predictions'].sum(),
                             'altitude_loss': 0.0 * output['predictions'].sum(), 'velocity_loss': 0.0 * output['predictions'].sum(),
                             'mindist_loss': 0.0 * output['predictions'].sum()}

                # ç´¯è®¡æŸå¤±
                for loss_name in total_losses:
                    total_losses[loss_name] += losses[loss_name].item()
                num_batches += 1

                # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡ç”¨äºæ€§èƒ½æŒ‡æ ‡è®¡ç®—
                # å–egoé£æœºçš„é¢„æµ‹å’Œç›®æ ‡ (batch_size, pred_len, features)
                predictions = output['predictions']
                targets = batch['targets']

                if targets.numel() > 0:  # ç¡®ä¿ä¸æ˜¯ç©ºæ ·æœ¬
                    # å–ç¬¬ä¸€æ¶é£æœº(ego)çš„é¢„æµ‹å’Œç›®æ ‡
                    ego_predictions = predictions[:, 0, :, :].cpu().numpy()  # [batch_size, pred_len, features]
                    ego_targets = targets[:, 0, :, :].cpu().numpy()          # [batch_size, pred_len, features]

                    all_predictions.append(ego_predictions)
                    all_targets.append(ego_targets)

                    # æ”¶é›†mindisté¢„æµ‹ (å‡è®¾mindistæ˜¯ç¬¬4ä¸ªé¢„æµ‹å¤´)
                    if 'mindist' in output:
                        mindist_pred = output['mindist'][:, 0, :, 0].cpu().numpy()  # [batch_size, pred_len]
                        all_mindist_predictions.append(mindist_pred)

                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'Loss': f"{losses['total_loss'].item():.6f}",
                    'Pos': f"{losses['position_loss'].item():.6f}",
                    'Alt': f"{losses['altitude_loss'].item():.6f}",
                    'Vel': f"{losses['velocity_loss'].item():.6f}",
                    'MD': f"{losses['mindist_loss'].item():.6f}"
                })

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_losses = {loss_name: total_loss / num_batches if num_batches > 0 else 0.0
                     for loss_name, total_loss in total_losses.items()}

        # ä¿æŒå‘åå…¼å®¹æ€§
        self.val_losses.append(avg_losses['total_loss'])

        # è®°å½•åˆ°TensorBoard
        self.writer.add_scalar('Validation/Total_Loss', avg_losses['total_loss'], self.epoch)

        # === è®¡ç®—æ€§èƒ½æŒ‡æ ‡ (Geminiå»ºè®®) ===
        performance_metrics = {}

        if all_predictions and all_targets:
            # åˆå¹¶æ‰€æœ‰batchçš„é¢„æµ‹å’Œç›®æ ‡
            all_predictions = np.concatenate(all_predictions, axis=0)  # [total_samples, pred_len, features]
            all_targets = np.concatenate(all_targets, axis=0)          # [total_samples, pred_len, features]

            # æ ¹æ®æ•°æ®é›†ç‰¹å¾é¡ºåºè®¡ç®—æŒ‡æ ‡
            # å‡è®¾ç‰¹å¾é¡ºåºä¸º: [flight_level, latitude, longitude, vx, vy] æˆ–ç±»ä¼¼
            try:
                # ä½ç½®æŒ‡æ ‡ (latitude, longitude) - å‡è®¾æ˜¯ç‰¹å¾1å’Œ2
                position_rmse = calculate_rmse(all_predictions, all_targets, feature_indices=[1, 2])
                position_mae = calculate_mae(all_predictions, all_targets, feature_indices=[1, 2])

                # é«˜åº¦æŒ‡æ ‡ (flight_level) - å‡è®¾æ˜¯ç‰¹å¾0
                altitude_rmse = calculate_rmse(all_predictions, all_targets, feature_indices=[0])
                altitude_mae = calculate_mae(all_predictions, all_targets, feature_indices=[0])

                # é€Ÿåº¦æŒ‡æ ‡ (vx, vy) - å‡è®¾æ˜¯ç‰¹å¾3å’Œ4
                velocity_rmse = calculate_rmse(all_predictions, all_targets, feature_indices=[3, 4])
                velocity_mae = calculate_mae(all_predictions, all_targets, feature_indices=[3, 4])

                performance_metrics.update({
                    'val_position_rmse': position_rmse,
                    'val_position_mae': position_mae,
                    'val_altitude_rmse': altitude_rmse,
                    'val_altitude_mae': altitude_mae,
                    'val_velocity_rmse': velocity_rmse,
                    'val_velocity_mae': velocity_mae
                })

                # è®¡ç®—è™šè­¦ç‡ (FAR) - ç¤¾äº¤æ¨¡å‹ç‰¹æœ‰æŒ‡æ ‡
                far = calculate_far(all_predictions, all_targets)
                performance_metrics['val_far'] = far

            except Exception as e:
                self.logger.warning(f"æ€§èƒ½æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
                # è®¾ç½®é»˜è®¤å€¼
                performance_metrics.update({
                    'val_position_rmse': 0.0,
                    'val_position_mae': 0.0,
                    'val_altitude_rmse': 0.0,
                    'val_altitude_mae': 0.0,
                    'val_velocity_rmse': 0.0,
                    'val_velocity_mae': 0.0,
                    'val_far': 0.0
                })

        # MindistæŒ‡æ ‡
        if all_mindist_predictions:
            all_mindist_predictions = np.concatenate(all_mindist_predictions, axis=0)
            performance_metrics['val_mindist_mean'] = float(np.mean(all_mindist_predictions))
        else:
            performance_metrics['val_mindist_mean'] = 0.0

        # åˆå¹¶æŸå¤±å’Œæ€§èƒ½æŒ‡æ ‡
        final_metrics = {**avg_losses, **performance_metrics}

        return final_metrics

    def save_checkpoint(self, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'best_val_loss': self.best_val_loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'config': self.config.config
        }

        # åˆ›å»ºcheckpointsç›®å½•
        checkpoints_dir = './checkpoints'
        os.makedirs(checkpoints_dir, exist_ok=True)

        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(checkpoints_dir, 'latest_checkpoint.pth')
        torch.save(checkpoint, checkpoint_path)

        # ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°checkpoints/
        if is_best:
            best_path = os.path.join(checkpoints_dir, 'best_model.pth')
            torch.save(checkpoint, best_path)
            self.logger.info(f"ğŸ’ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {best_path}")

        # å®šæœŸä¿å­˜
        save_freq = self.config.get('logging.save_freq', 10)
        if (self.epoch + 1) % save_freq == 0:
            epoch_path = os.path.join(
                checkpoints_dir, f'checkpoint_epoch_{self.epoch+1}.pth'
            )
            torch.save(checkpoint, epoch_path)

    def save_metrics_to_json(self, filename: str = None):
        """ä¿å­˜æ‰€æœ‰è®­ç»ƒæŒ‡æ ‡åˆ°JSONæ–‡ä»¶"""
        if filename is None:
            # åˆ›å»ºmetrics_calculationç›®å½•
            metrics_dir = './metrics_calculation'
            os.makedirs(metrics_dir, exist_ok=True)
            filename = f'{metrics_dir}/training_metrics_epoch_{self.epoch}.json'

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filename), exist_ok=True)

        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        results_data = {
            'experiment_info': {
                'model_name': 'Social-PatchTST',
                'config_file': self.config.config_path if hasattr(self.config, 'config_path') else 'unknown',
                'total_epochs': self.epoch + 1,
                'best_epoch': self.metrics_history['val_total_loss'].index(min(self.metrics_history['val_total_loss'])) if self.metrics_history['val_total_loss'] else 0,
                'best_val_loss': min(self.metrics_history['val_total_loss']) if self.metrics_history['val_total_loss'] else float('inf'),
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            },
            'final_metrics': {
                'train_total_loss': self.metrics_history['train_total_loss'][-1] if self.metrics_history['train_total_loss'] else 0.0,
                'val_total_loss': self.metrics_history['val_total_loss'][-1] if self.metrics_history['val_total_loss'] else 0.0,
                'train_position_loss': self.metrics_history['train_position_loss'][-1] if self.metrics_history['train_position_loss'] else 0.0,
                'val_position_loss': self.metrics_history['val_position_loss'][-1] if self.metrics_history['val_position_loss'] else 0.0,
                'val_position_rmse': self.metrics_history['val_position_rmse'][-1] if self.metrics_history['val_position_rmse'] else 0.0,
                'val_position_mae': self.metrics_history['val_position_mae'][-1] if self.metrics_history['val_position_mae'] else 0.0,
                'val_altitude_rmse': self.metrics_history['val_altitude_rmse'][-1] if self.metrics_history['val_altitude_rmse'] else 0.0,
                'val_velocity_rmse': self.metrics_history['val_velocity_rmse'][-1] if self.metrics_history['val_velocity_rmse'] else 0.0,
                'val_far': self.metrics_history['val_far'][-1] if self.metrics_history['val_far'] else 0.0,
                'final_learning_rate': self.metrics_history['learning_rates'][-1] if self.metrics_history['learning_rates'] else 0.0
            },
            'best_epoch_metrics': {},
            'metrics_history': self.metrics_history
        }

        # æ·»åŠ æœ€ä½³epochçš„æŒ‡æ ‡
        if self.metrics_history['val_total_loss']:
            best_epoch = results_data['experiment_info']['best_epoch']
            results_data['best_epoch_metrics'] = {
                'epoch': best_epoch,
                'train_total_loss': self.metrics_history['train_total_loss'][best_epoch] if best_epoch < len(self.metrics_history['train_total_loss']) else 0.0,
                'val_total_loss': self.metrics_history['val_total_loss'][best_epoch],
                'val_position_rmse': self.metrics_history['val_position_rmse'][best_epoch] if best_epoch < len(self.metrics_history['val_position_rmse']) else 0.0,
                'val_position_mae': self.metrics_history['val_position_mae'][best_epoch] if best_epoch < len(self.metrics_history['val_position_mae']) else 0.0,
                'val_altitude_rmse': self.metrics_history['val_altitude_rmse'][best_epoch] if best_epoch < len(self.metrics_history['val_altitude_rmse']) else 0.0,
                'val_velocity_rmse': self.metrics_history['val_velocity_rmse'][best_epoch] if best_epoch < len(self.metrics_history['val_velocity_rmse']) else 0.0,
                'val_far': self.metrics_history['val_far'][best_epoch] if best_epoch < len(self.metrics_history['val_far']) else 0.0,
                'learning_rate': self.metrics_history['learning_rates'][best_epoch] if best_epoch < len(self.metrics_history['learning_rates']) else 0.0
            }

        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)

        self.logger.info(f"ğŸ“Š è®­ç»ƒæŒ‡æ ‡å·²ä¿å­˜åˆ°: {filename}")

        # æ‰“å°æœ€ç»ˆç»“æœæ‘˜è¦
        self.logger.info("=== è®­ç»ƒå®Œæˆæ‘˜è¦ ===")
        self.logger.info(f"æ€»è®­ç»ƒè½®æ•°: {results_data['experiment_info']['total_epochs']}")
        self.logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {results_data['experiment_info']['best_val_loss']:.6f} (epoch {results_data['experiment_info']['best_epoch']})")
        if results_data['best_epoch_metrics']:
            self.logger.info(f"æœ€ä½³è½®æ¬¡ä½ç½®RMSE: {results_data['best_epoch_metrics']['val_position_rmse']:.6f}")
            self.logger.info(f"æœ€ä½³è½®æ¬¡è™šè­¦ç‡: {results_data['best_epoch_metrics']['val_far']:.6f}")

        return filename

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        training_config = self.config.get('training', {})
        epochs = training_config.get('epochs', 100)
        patience = training_config.get('patience', 10)

        self.logger.info(f"å¼€å§‹è®­ç»ƒï¼Œæ€»epochs: {epochs}")

        # æ—©åœè®¡æ•°å™¨
        patience_counter = 0

        for epoch in range(epochs):
            self.epoch = epoch
            epoch_start_time = time.time()

            # è®­ç»ƒ
            train_metrics = self.train_epoch()
            self.logger.info(f"Epoch {epoch+1} - Train Loss: {train_metrics['total_loss']:.6f}")

            # éªŒè¯
            val_metrics = self.validate_epoch()
            self.logger.info(f"Epoch {epoch+1} - Val Loss: {val_metrics['total_loss']:.6f}")

            # è®°å½•epochæ—¶é—´
            epoch_time = time.time() - epoch_start_time

            # === è®°å½•æ‰€æœ‰æŒ‡æ ‡åˆ°å†å² (Geminiå»ºè®®) ===
            self.metrics_history['train_total_loss'].append(train_metrics['total_loss'])
            self.metrics_history['train_position_loss'].append(train_metrics['position_loss'])
            self.metrics_history['train_altitude_loss'].append(train_metrics['altitude_loss'])
            self.metrics_history['train_velocity_loss'].append(train_metrics['velocity_loss'])
            self.metrics_history['train_mindist_loss'].append(train_metrics['mindist_loss'])

            self.metrics_history['val_total_loss'].append(val_metrics['total_loss'])
            self.metrics_history['val_position_loss'].append(val_metrics['position_loss'])
            self.metrics_history['val_altitude_loss'].append(val_metrics['altitude_loss'])
            self.metrics_history['val_velocity_loss'].append(val_metrics['velocity_loss'])
            self.metrics_history['val_mindist_loss'].append(val_metrics['mindist_loss'])

            # æ€§èƒ½æŒ‡æ ‡ (å¦‚æœæœ‰çš„è¯)
            self.metrics_history['val_position_rmse'].append(val_metrics.get('val_position_rmse', 0.0))
            self.metrics_history['val_altitude_rmse'].append(val_metrics.get('val_altitude_rmse', 0.0))
            self.metrics_history['val_velocity_rmse'].append(val_metrics.get('val_velocity_rmse', 0.0))
            self.metrics_history['val_position_mae'].append(val_metrics.get('val_position_mae', 0.0))
            self.metrics_history['val_altitude_mae'].append(val_metrics.get('val_altitude_mae', 0.0))
            self.metrics_history['val_velocity_mae'].append(val_metrics.get('val_velocity_mae', 0.0))

            # ç¤¾äº¤æ¨¡å‹ç‰¹æœ‰æŒ‡æ ‡
            self.metrics_history['val_far'].append(val_metrics.get('val_far', 0.0))
            self.metrics_history['val_mindist_mean'].append(val_metrics.get('val_mindist_mean', 0.0))

            # è®­ç»ƒä¿¡æ¯
            self.metrics_history['epoch_times'].append(epoch_time)

            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step()
                current_lr = self.optimizer.param_groups[0]['lr']
                self.metrics_history['learning_rates'].append(current_lr)
                self.writer.add_scalar('Learning_Rate', current_lr, epoch)
                self.logger.info(f"Learning Rate: {current_lr:.8f}")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_metrics['total_loss'] < self.best_val_loss
            if is_best:
                self.best_val_loss = val_metrics['total_loss']
                patience_counter = 0
                self.logger.info(f"æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {val_metrics['total_loss']:.6f}")
            else:
                patience_counter += 1

            self.save_checkpoint(is_best)

            # æ¯5è½®ä¿å­˜ä¸€æ¬¡æŒ‡æ ‡JSON
            if (epoch + 1) % 5 == 0:
                self.save_metrics_to_json(f'./metrics_calculation/training_metrics_epoch_{epoch+1}.json')

            # æ—©åœ
            if patience_counter >= patience:
                self.logger.info(f"æ—©åœè§¦å‘ï¼Œpatience: {patience}")
                break

        # è®­ç»ƒå®Œæˆåï¼Œä¿å­˜æœ€ç»ˆçš„å®Œæ•´æŒ‡æ ‡
        final_metrics_file = self.save_metrics_to_json('./metrics_calculation/final_training_metrics.json')

        self.logger.info("è®­ç»ƒå®Œæˆï¼")

        return final_metrics_file


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒSocial-PatchTSTæ¨¡å‹')
    parser.add_argument('--config', type=str,
                       default='config/social_patchtst_config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', action='store_true',
                       help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    parser.add_argument('--test', action='store_true',
                       help='è¿è¡Œæµ‹è¯•æ¨¡å¼è€Œä¸æ˜¯è®­ç»ƒ')
    parser.add_argument('--scenes_dir', type=str,
                       help='åœºæ™¯æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--baseline', action='store_true',
                       help='è¿è¡ŒåŸç‰ˆPatchTST (Baseline) æ¨¡å¼ï¼Œå…³é—­ç¤¾äº¤æ¨¡å—')

    args = parser.parse_args()

    if not os.path.exists(args.config):
        print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return

    # å¦‚æœæŒ‡å®šäº†æµ‹è¯•æ¨¡å¼
    if args.test:
        from data.dataset.scene_dataset import SocialPatchTSTDataset
        from config.config_manager import load_config

        config = load_config(args.config)
        scenes_dir = args.scenes_dir or config.data_config.get('scenes_dir', '/tmp/test_scenes')

        print(f"æµ‹è¯•åœºæ™¯æ•°æ®åŠ è½½...")
        print(f"åœºæ™¯ç›®å½•: {scenes_dir}")

        # For testing, we need a paths file
        paths_file = None
        if os.path.exists(os.path.join(scenes_dir, "train_paths.txt")):
            paths_file = os.path.join(scenes_dir, "train_paths.txt")

        dataset = SocialPatchTSTDataset(
            data_dir=scenes_dir,
            max_neighbors=10,
            paths_file=paths_file
        )

        print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")

        if len(dataset) > 0:
            sample = dataset[0]
            print(f"æ ·æœ¬å½¢çŠ¶:")
            print(f"  - æ—¶åºæ•°æ®: {sample['temporal'].shape}")
            print(f"  - ç©ºé—´æ•°æ®: {sample['spatial'].shape}")
            print(f"  - ç›®æ ‡æ•°æ®: {sample['targets'].shape}")
            print(f"  - è·ç¦»çŸ©é˜µ: {sample['distance_matrix'].shape}")

        print("âœ… æµ‹è¯•å®Œæˆ!")
        return

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(args.config, is_baseline=args.baseline)

    # å¦‚æœæŒ‡å®šäº†æ¢å¤è®­ç»ƒï¼ŒåŠ è½½æ£€æŸ¥ç‚¹
    if args.resume:
        checkpoint_path = './checkpoints/latest_checkpoint.pth'
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path, map_location=trainer.device)
            trainer.model.load_state_dict(checkpoint['model_state_dict'])
            trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if trainer.scheduler and checkpoint['scheduler_state_dict']:
                trainer.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            trainer.epoch = checkpoint['epoch']
            trainer.best_val_loss = checkpoint['best_val_loss']
            trainer.train_losses = checkpoint['train_losses']
            trainer.val_losses = checkpoint['val_losses']
            trainer.logger.info(f"ä»epoch {trainer.epoch}æ¢å¤è®­ç»ƒï¼Œæœ€ä½³éªŒè¯æŸå¤±: {trainer.best_val_loss:.6f}")
        else:
            trainer.logger.warning("æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")

    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == "__main__":
    main()